\documentclass[12pt,a4paper]{report}
\usepackage[utf8]{inputenc}
\usepackage[italian,english]{babel}
\usepackage{geometry}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{longtable}
\usepackage{float}
\usepackage{subcaption}
\usepackage{tikz}
\usepackage{pgfplots}
\pgfplotsset{compat=1.18}

% Configurazione pagina
\geometry{
    left=2.5cm,
    right=2.5cm,
    top=2.5cm,
    bottom=2.5cm
}

% Configurazione hyperref
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,
    urlcolor=cyan,
    citecolor=green
}

% Configurazione listings per codice
\lstdefinestyle{mystyle}{
    backgroundcolor=\color{gray!10},
    commentstyle=\color{green!60!black},
    keywordstyle=\color{blue},
    numberstyle=\tiny\color{gray},
    stringstyle=\color{orange},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,
    breaklines=true,
    captionpos=b,
    keepspaces=true,
    numbers=left,
    numbersep=5pt,
    showspaces=false,
    showstringspaces=false,
    showtabs=false,
    tabsize=2,
    frame=single,
    rulecolor=\color{gray!30}
}
\lstset{style=mystyle}

% Titolo e autore
\title{
    \vspace{2cm}
    \textbf{\Huge Sistema Avanzato per il Rilevamento e l'Analisi\\di Disinformazione Medica}\\
    \vspace{1cm}
    \Large Documentazione Tecnica Completa\\
    \vspace{0.5cm}
    \large Versione 1.0
    \vspace{2cm}
}

\author{
    Michele Spinelli\\
    \vspace{0.5cm}
    \textit{Università degli Studi}\\
    \vspace{0.3cm}
    \small \href{mailto:email@university.edu}{email@university.edu}
}

\date{\today}

\begin{document}

% Pagina del titolo
\maketitle

% Indice
\tableofcontents
\newpage
\listoffigures
\newpage
\listoftables
\newpage

%=====================================
% EXECUTIVE SUMMARY
%=====================================
\chapter{Executive Summary}

\section{Il Fenomeno della Disinformazione Medica nell'Era Digitale}

\subsection{Contesto Globale e Rilevanza del Problema}

La disinformazione medica rappresenta oggi una delle minacce più significative per la salute pubblica globale, amplificata in modo esponenziale dall'avvento dei social media e dalla democratizzazione dell'informazione digitale. L'Organizzazione Mondiale della Sanità (OMS) ha ufficialmente riconosciuto l'\textit{infodemia} come una vera e propria emergenza sanitaria parallela alle crisi epidemiologiche fisiche, definendola come "un eccesso di informazioni, alcune accurate e altre no, che rendono difficile per le persone trovare fonti affidabili e orientamenti fidati quando ne hanno bisogno".

Il fenomeno ha raggiunto proporzioni critiche durante la pandemia di COVID-19, quando la diffusione di informazioni false e fuorvianti ha contribuito significativamente a:

\begin{itemize}
    \item \textbf{Riduzione dell'adesione alle misure preventive}: Studi epidemiologici hanno dimostrato una correlazione diretta tra esposizione a disinformazione e ridotta compliance alle raccomandazioni sanitarie
    \item \textbf{Esitazione vaccinale}: Il fenomeno dell'hesitancy vaccinale è stato alimentato dalla circolazione di teorie cospirative e informazioni scientificamente infondate
    \item \textbf{Automedicazione pericolosa}: Promozione di trattamenti non validati, dalla clorochina ai rimedi casalinghi potenzialmente dannosi
    \item \textbf{Erosione della fiducia istituzionale}: Deterioramento progressivo della credibilità delle autorità sanitarie e del sistema scientifico
\end{itemize}

La gravità del fenomeno è sottolineata da dati allarmanti: secondo un'analisi condotta da Reuters Institute nel 2021, il 25\% di tutti i decessi correlati alla disinformazione COVID-19 documentati era direttamente attribuibile a comportamenti influenzati da informazioni false circolate online.

\subsection{Il Ruolo Amplificatore dei Social Media}

I social media hanno rivoluzionato il panorama informativo, trasformandosi da semplici piattaforme di condivisione a potenti vettori di influenza comportamentale. Questa trasformazione presenta caratteristiche peculiari che rendono particolarmente insidiosa la diffusione di disinformazione medica:

\subsubsection{Meccanismi di Amplificazione Algoritmica}

Gli algoritmi di raccomandazione dei social media sono progettati per massimizzare l'engagement degli utenti, privilegiando contenuti che generano reazioni emotive intense. Questo meccanismo presenta bias intrinseci:

\begin{itemize}
    \item \textbf{Bias della novità}: I contenuti "sensazionali" ricevono maggiore visibilità rispetto a informazioni accurate ma meno emotivamente coinvolgenti
    \item \textbf{Echo chambers}: Gli algoritmi tendono a mostrare contenuti allineati con le convinzioni preesistenti, creando "camere di risonanza" che rafforzano bias cognitivi
    \item \textbf{Velocità vs. accuratezza}: La pressione per la condivisione immediata privilegia la rapidità rispetto alla verifica delle fonti
\end{itemize}

\subsubsection{Democratizzazione dell'Autorità Informativa}

I social media hanno eliminato i tradizionali gatekeepers dell'informazione, consentendo a chiunque di assumere il ruolo di "esperto" sanitario. Questo fenomeno si manifesta attraverso:

\begin{itemize}
    \item \textbf{Pseudo-autorevolezza}: Utilizzo di linguaggio tecnico e riferimenti distorti a ricerche scientifiche per conferire credibilità a informazioni false
    \item \textbf{Personalizzazione del messaggio}: Adattamento del contenuto ai timori e alle speranze specifiche del target audience
    \item \textbf{Sfruttamento della complessità}: Semplificazione eccessiva di questioni mediche complesse, offrendo "soluzioni" apparentemente semplici a problemi complessi
\end{itemize}

\subsubsection{Network Effects e Diffusione Virale}

La struttura reticolare dei social media facilita la diffusione esponenziale di contenuti attraverso:

\begin{itemize}
    \item \textbf{Condivisione acritica}: Gli utenti tendono a condividere contenuti basandosi su titoli accattivanti senza verificarne l'accuratezza
    \item \textbf{Amplificazione sociale}: Il numero di condivisioni viene interpretato come indicatore di veridicità
    \item \textbf{Cross-platform contamination}: I contenuti si diffondono rapidamente tra diverse piattaforme, moltiplicando la loro portata
\end{itemize}

\subsection{Impatti Documentati sulla Salute Pubblica}

La ricerca scientifica ha documentato impatti concreti e misurabili della disinformazione medica:

\subsubsection{Impatti Epidemiologici}

\begin{itemize}
    \item \textbf{Riduzione della copertura vaccinale}: Correlazione documentata tra intensità della disinformazione anti-vaccinale e riduzione delle vaccinazioni di routine
    \item \textbf{Ritardo nella ricerca di cure}: Pazienti che procrastinano trattamenti medici essenziali a causa di informazioni fuorvianti sui rischi
    \item \textbf{Aumento di comportamenti a rischio}: Adozione di pratiche dannose promosse come "alternative naturali" a trattamenti scientificamente validati
\end{itemize}

\subsubsection{Impatti Socio-Economici}

\begin{itemize}
    \item \textbf{Costi sanitari diretti}: Aumento dei ricoveri ospedalieri dovuti a automedicazione inappropriata
    \item \textbf{Perdita di produttività}: Assenze lavorative prolungate dovute a mancata adesione a trattamenti efficaci
    \item \textbf{Costi di comunicazione}: Risorse significative destinate dalle istituzioni sanitarie per contrastare disinformazione
\end{itemize}

\subsubsection{Impatti Psicologici e Sociali}

\begin{itemize}
    \item \textbf{Ansia sanitaria}: Aumento dei livelli di ansia e paranoia relativi a questioni mediche
    \item \textbf{Polarizzazione sociale}: Creazione di divisioni nella società basate su credenze sanitarie non evidenziate scientificamente
    \item \textbf{Erosione del dialogo medico-paziente}: Deterioramento della comunicazione tra professionisti sanitari e pazienti
\end{itemize}

\section{La Necessità di Sistemi di Monitoraggio Automatizzati}

\subsection{Limitazioni degli Approcci Tradizionali}

I meccanismi tradizionali di controllo dell'informazione medica si sono rivelati inadeguati di fronte alla scala e alla velocità del fenomeno digitale:

\subsubsection{Fact-Checking Manuale}

\begin{itemize}
    \item \textbf{Scalabilità limitata}: I fact-checker professionali possono analizzare solo una frazione infinitesimale dei contenuti prodotti quotidianamente
    \item \textbf{Tempi di risposta}: Il processo di verifica manuale richiede ore o giorni, mentre la disinformazione si diffonde in minuti
    \item \textbf{Expertise specializzata}: Richiede competenze mediche specifiche non sempre disponibili nei team di fact-checking generalisti
    \item \textbf{Bias cognitivi}: Anche gli esperti umani possono essere influenzati da pregiudizi personali o pressioni esterne
\end{itemize}

\subsubsection{Moderazione delle Piattaforme}

\begin{itemize}
    \item \textbf{Approccio reattivo}: Intervento solo dopo segnalazioni multiple, quando il contenuto ha già raggiunto vasta diffusione
    \item \textbf{Inconsistenza}: Applicazione disomogenea delle policy tra diverse piattaforme e contesti culturali
    \item \textbf{Problemi di trasparenza}: Algoritmi di moderazione non trasparenti e difficilmente auditabili
    \item \textbf{Equilibrio libertà-sicurezza}: Tensione costante tra libertà di espressione e protezione dalla disinformazione
\end{itemize}

\subsubsection{Controllo Istituzionale}

\begin{itemize}
    \item \textbf{Autorità limitata}: Le istituzioni sanitarie non hanno controllo diretto sui canali di diffusione dell'informazione
    \item \textbf{Credibilità compromessa}: Paradossalmente, l'autorevolezza delle istituzioni è essa stessa sotto attacco dalla disinformazione
    \item \textbf{Comunicazione inefficace}: Linguaggio tecnico-scientifico spesso poco accessibile al pubblico generale
\end{itemize}

\subsection{Requisiti per un Sistema di Monitoraggio Efficace}

L'analisi delle limitazioni esistenti evidenzia la necessità di un approccio tecnologico innovativo che presenti le seguenti caratteristiche:

\subsubsection{Scalabilità e Velocità}

\begin{itemize}
    \item \textbf{Monitoraggio continuo}: Capacità di analizzare flussi di dati in tempo reale da multiple piattaforme
    \item \textbf{Elaborazione massiva}: Gestione di milioni di contenuti giornalieri con latenza minima
    \item \textbf{Rilevamento precoce}: Identificazione di contenuti problematici prima della diffusione virale
\end{itemize}

\subsubsection{Accuratezza e Affidabilità}

\begin{itemize}
    \item \textbf{Validazione multi-fonte}: Confronto automatico con database scientifici autorevoli
    \item \textbf{Riduzione dei bias}: Utilizzo di multiple fonti di verità e algoritmi trasparenti
    \item \textbf{Spiegabilità}: Capacità di fornire motivazioni comprensibili per le valutazioni automatiche
\end{itemize}

\subsubsection{Adattabilità e Apprendimento}

\begin{itemize}
    \item \textbf{Evoluzione continua}: Capacità di adattarsi a nuove forme di disinformazione e tattiche evasive
    \item \textbf{Personalizzazione contestuale}: Adattamento a diversi domini medici e contesti culturali
    \item \textbf{Feedback loop}: Integrazione del feedback umano per il miglioramento continuo del sistema
\end{itemize}

\section{Panoramica della Soluzione Proposta}

\subsection{Approccio Innovativo Multi-Modale}

Il sistema sviluppato rappresenta una risposta tecnologica avanzata alle sfide identificate, integrando:

\begin{itemize}
    \item \textbf{Intelligenza Artificiale Distribuita}: Utilizzo coordinato di multiple LLM (Large Language Models) per ridurre bias e aumentare robustezza
    \item \textbf{Validazione Scientifica Automatizzata}: Integrazione diretta con database PubMed e fonti istituzionali per verifica in tempo reale
    \item \textbf{Analisi Linguistica Avanzata}: NLP specializzato per il dominio medico-sanitario con focus sulla lingua italiana
    \item \textbf{Monitoraggio Cross-Platform}: Raccolta coordinata da Twitter, Facebook, Reddit, YouTube, Instagram e Telegram
\end{itemize}

\subsection{Obiettivi Strategici del Sistema}

\subsubsection{Obiettivi Primari}

\begin{itemize}
    \item \textbf{Rilevamento Automatico}: Identificazione automatica di contenuti medici potenzialmente dannosi con precisione > 85\%
    \item \textbf{Classificazione Graduale}: Sistema di scoring granulare (0-4) per quantificare il livello di disinformazione
    \item \textbf{Tracciabilità Scientifica}: Collegamento automatico a evidenze scientifiche per ogni valutazione
    \item \textbf{Analisi di Diffusione}: Monitoraggio dei pattern di propagazione e identificazione di super-spreaders
\end{itemize}

\subsubsection{Obiettivi di Ricerca}

\begin{itemize}
    \item \textbf{Dataset Annotato}: Creazione di un corpus di riferimento per la ricerca sulla disinformazione medica italiana
    \item \textbf{Metodologie Innovative}: Sviluppo di tecniche di fact-checking ibride umano-AI
    \item \textbf{Analisi Epidemiologica}: Studio dei meccanismi di diffusione della disinformazione medica
    \item \textbf{Impatto Comportamentale}: Correlazione tra esposizione a disinformazione e comportamenti sanitari
\end{itemize}

\subsubsection{Obiettivi di Supporto Istituzionale}

\begin{itemize}
    \item \textbf{Early Warning System}: Allerta precoce per istituzioni sanitarie su nuove forme di disinformazione
    \item \textbf{Evidence-Based Communication}: Supporto per campagne di comunicazione basate su evidenze
    \item \textbf{Policy Support}: Fornitura di dati per informed decision making nelle politiche sanitarie
    \item \textbf{Formazione Professionale}: Strumenti per l'aggiornamento dei professionisti sanitari sui trend informativi
\end{itemize}

\subsection{Innovazioni Tecnologiche Chiave}

\subsubsection{Architettura Ibrida AI-Umano}

Il sistema implementa un approccio innovativo che combina:

\begin{itemize}
    \item \textbf{Pre-screening Automatico}: Analisi AI di primo livello per identificare contenuti sospetti
    \item \textbf{Validazione Multi-LLM}: Verifica incrociata utilizzando diversi modelli linguistici per ridurre bias
    \item \textbf{Human-in-the-loop}: Integrazione di expertise umana per casi complessi e training del sistema
    \item \textbf{Continuous Learning}: Adattamento dinamico basato su feedback e nuovi pattern identificati
\end{itemize}

\subsubsection{Integrazione Scientifica Real-time}

\begin{itemize}
    \item \textbf{PubMed API Integration}: Accesso diretto al database bibliografico biomedico più autorevole
    \item \textbf{Institutional Feeds}: Monitoraggio di comunicazioni ufficiali da OMS, ISS, EMA, FDA
    \item \textbf{Semantic Matching}: Algoritmi avanzati per il matching semantico tra claim e evidenze scientifiche
    \item \textbf{Evidence Ranking}: Sistema di ranking delle evidenze basato su qualità metodologica e recency
\end{itemize}

\subsubsection{Analisi Multimodale}

\begin{itemize}
    \item \textbf{Textual Analysis}: NLP avanzato per l'analisi del contenuto testuale
    \item \textbf{Network Analysis}: Studio dei pattern di diffusione e identificazione di cluster
    \item \textbf{Temporal Analysis}: Analisi dell'evoluzione temporale dei narrative di disinformazione
    \item \textbf{Cross-Platform Correlation}: Identificazione di campagne coordinate su multiple piattaforme
\end{itemize}

\section{Impatti Attesi e Beneficiari}

\subsection{Comunità Scientifica e Ricerca}

\begin{itemize}
    \item \textbf{Accelerazione della Ricerca}: Riduzione dei tempi per identificare e studiare nuovi fenomeni di disinformazione
    \item \textbf{Metodologie Innovative}: Sviluppo di nuovi approcci per lo studio dell'infodemia
    \item \textbf{Collaborazione Interdisciplinare}: Facilitazione della collaborazione tra informatici, epidemiologi, psicologi e comunicatori
    \item \textbf{Pubblicazioni e Brevetti}: Opportunità per pubblicazioni scientifiche di alto impatto e potenziali brevetti tecnologici
\end{itemize}

\subsection{Istituzioni Sanitarie}

\begin{itemize}
    \item \textbf{Monitoraggio Proattivo}: Capacità di anticipare e prepararsi a nuove ondate di disinformazione
    \item \textbf{Comunicazione Targettizzata}: Sviluppo di strategie comunicative basate su dati real-time
    \item \textbf{Resource Optimization}: Allocazione più efficiente delle risorse comunicative
    \item \textbf{Crisis Management}: Miglioramento della gestione comunicativa durante emergenze sanitarie
\end{itemize}

\subsection{Società Civile}

\begin{itemize}
    \item \textbf{Protezione della Salute Pubblica}: Riduzione dell'esposizione a informazioni mediche dannose
    \item \textbf{Empowerment Informativo}: Migliore capacità dei cittadini di distinguere informazioni affidabili
    \item \textbf{Riduzione dell'Ansia Sanitaria}: Diminuzione dell'incertezza dovuta a informazioni contrastanti
    \item \textbf{Migliore Literacy Sanitaria}: Aumento della competenza sanitaria generale della popolazione
\end{itemize}

\subsection{Ecosistema Tecnologico}

\begin{itemize}
    \item \textbf{Standard Settoriali}: Contributo allo sviluppo di standard per il fact-checking automatico
    \item \textbf{Piattaforme Social}: Supporto per migliorare gli algoritmi di moderazione delle piattaforme
    \item \textbf{Startup e Innovation}: Opportunità per lo sviluppo di soluzioni commerciali derivate
    \item \textbf{Trasferimento Tecnologico}: Potenziale per l'applicazione delle tecnologie sviluppate ad altri domini
\end{itemize}

\section{Architettura e Tecnologie}

\subsection{Stack Tecnologico}

Il sistema adotta un'architettura moderna e scalabile basata su:

\textbf{Frontend:}
\begin{itemize}
    \item React 19.1.1 con hooks moderni
    \item React Router per Single Page Application (SPA)
    \item Tailwind CSS per styling responsive
    \item Recharts per visualizzazioni dati
    \item React Force Graph per network analysis
\end{itemize}

\textbf{Backend:}
\begin{itemize}
    \item Flask 3.1.1 come framework web Python
    \item MongoDB come database NoSQL principale
    \item Elasticsearch per indicizzazione e ricerca avanzata
    \item Redis per caching e session management
    \item Celery per task asincroni
\end{itemize}

\textbf{AI e Machine Learning:}
\begin{itemize}
    \item Integrazione multi-provider LLM (OpenAI GPT, Anthropic Claude, Google Gemini)
    \item spaCy per NLP avanzato con modelli italiani
    \item Sentence Transformers per embedding semantici
    \item Scikit-learn per analisi statistica
    \item NLTK per preprocessing linguistico
\end{itemize}

\subsection{Moduli Principali}

\begin{enumerate}
    \item \textbf{Data Collection Engine}: Raccolta automatizzata da API social
    \item \textbf{NLP Processing Pipeline}: Analisi linguistica e estrazione entità
    \item \textbf{AI Fact-Checking Engine}: Valutazione automatica veridicità
    \item \textbf{Hybrid Retrieval System}: Sistema ibrido per recupero informazioni
    \item \textbf{Campaign Management}: Gestione campagne di monitoraggio
    \item \textbf{Reporting & Analytics}: Dashboard e export report
\end{enumerate}

\section{Benefici e Risultati Attesi}

\subsection{Benefici per la Ricerca}
\begin{itemize}
    \item Accelerazione della ricerca sulla disinformazione medica
    \item Dataset annotati per training di modelli ML
    \item Analisi di trend e pattern di diffusione
    \item Supporto per studi epidemiologici digitali
\end{itemize}

\subsection{Benefici per le Istituzioni Sanitarie}
\begin{itemize}
    \item Early warning system per emergenze sanitarie
    \item Monitoraggio sentiment pubblico su politiche sanitarie
    \item Identificazione di cluster di disinformazione
    \item Supporto per campagne di comunicazione evidenze-based
\end{itemize}

\subsection{Metriche di Performance}
\begin{itemize}
    \item Capacità di processamento: 10,000+ post/ora
    \item Tempo di risposta medio: < 2 secondi per analisi
    \item Accuratezza fact-checking: 85-90\% (baseline)
    \item Copertura linguistica: Italiano con supporto multilingua
    \item Disponibilità sistema: 99.5\% uptime target
\end{itemize}

%=====================================
% OBIETTIVI E METODOLOGIE DEL PROGETTO
%=====================================
\chapter{Obiettivi e Metodologie del Progetto}

\section{Obiettivi del Progetto}

\subsection{Obiettivi Primari}

Il progetto si propone di sviluppare una piattaforma integrata per il monitoraggio e l'analisi automatizzata della disinformazione medica sui social media, con i seguenti obiettivi principali:

\subsubsection{OP-01: Sistema di Monitoraggio Multi-Platform}

\begin{itemize}
    \item \textbf{Raccolta Automatizzata}: Implementazione di data collectors per Twitter/X, Facebook, Reddit, YouTube, Instagram, Telegram
    \item \textbf{Real-time Processing}: Elaborazione in tempo reale di flussi di dati con latenza < 2 secondi
    \item \textbf{Scalabilità}: Capacità di processare 10,000+ post/ora con architettura distribuita
    \item \textbf{Filtering Intelligente}: Pre-selezione automatica di contenuti medico-sanitari
\end{itemize}

\subsubsection{OP-02: Engine di Fact-Checking Avanzato}

\begin{itemize}
    \item \textbf{Multi-LLM Orchestration}: Integrazione coordinata di GPT-4, Claude-3, Gemini per riduzione bias
    \item \textbf{Scientific Validation}: Verifica automatica con database PubMed e fonti istituzionali
    \item \textbf{Dual-Claim Analysis}: Distinzione tra claim generali e context-specific per accuratezza superiore
    \item \textbf{Explainable AI}: Sistema di scoring trasparente con citazioni delle fonti utilizzate
\end{itemize}

\subsubsection{OP-03: NLP Specializzato per Contenuti Medici Italiani}

\begin{itemize}
    \item \textbf{Domain Adaptation}: Modelli NLP fine-tuned su corpus medici italiani
    \item \textbf{Medical Entity Recognition}: Estrazione automatica di patologie, farmaci, procedure
    \item \textbf{Claim Detection}: Identificazione di statement verificabili vs opinioni soggettive
    \item \textbf{Sentiment e Polarization Analysis}: Analisi di tono emotivo e polarizzazione dei contenuti
\end{itemize}

\subsubsection{OP-04: Sistema di Campaign Management}

\begin{itemize}
    \item \textbf{Campaign Personalizzabili}: Configurazione di monitoraggio per keywords, piattaforme, timeframe specifici
    \item \textbf{User Management}: Sistema multi-utente con ruoli differenziati (ricercatore, istituzione, amministratore)
    \item \textbf{Reporting Avanzato}: Dashboard interattive e export in formati multipli (PDF, CSV, JSON)
    \item \textbf{Alert System}: Notifiche real-time per contenuti ad alto rischio
\end{itemize}

\subsection{Obiettivi Secondari}

\subsubsection{Contributi alla Ricerca Scientifica}

\begin{itemize}
    \item \textbf{Dataset Annotato}: Creazione di corpus di riferimento per disinformazione medica italiana
    \item \textbf{Metodologie Innovative}: Sviluppo di tecniche hybrid human-AI per fact-checking
    \item \textbf{Metriche Specifiche}: Definizione di KPI per valutazione accuratezza in dominio medico
    \item \textbf{Open Source Components}: Rilascio di componenti riutilizzabili per la community scientifica
\end{itemize}

\subsubsection{Supporto Istituzionale}

\begin{itemize}
    \item \textbf{Early Warning System}: Allerta precoce per istituzioni sanitarie su trend emergenti
    \item \textbf{Policy Support}: Dati e analisi per supportare decision making evidence-based
    \item \textbf{Public Communication}: Supporto per campagne di comunicazione scientifica mirata
    \item \textbf{Training Materials}: Risorse per formazione di operatori sanitari e comunicatori
\end{itemize}

\section{Workflow del Sistema}

\subsection{Pipeline di Processing End-to-End}

Il sistema implementa una pipeline di processing completa che trasforma i dati raw dei social media in analisi strutturate attraverso le seguenti fasi:

\subsubsection{Fase 1: Data Acquisition}

\begin{enumerate}
    \item \textbf{Campaign Configuration}:
    \begin{itemize}
        \item Definizione keywords mediche target
        \item Selezione piattaforme social da monitorare
        \item Configurazione parametri temporali e geografici
        \item Setup filtri di pre-selezione contenuti
    \end{itemize}
    
    \item \textbf{Multi-Platform Data Collection}:
    \begin{itemize}
        \item Connessione API native delle piattaforme social
        \item Rate limiting automatico per compliance API
        \item Parallel processing per massimizzare throughput
        \item Error handling e retry logic per robustezza
    \end{itemize}
    
    \item \textbf{Data Normalization}:
    \begin{itemize}
        \item Standardizzazione formato dati cross-platform
        \item Estrazione metadata uniformi (timestamp, author, engagement)
        \item Deduplication basata su content similarity
        \item Storage in MongoDB con indexing ottimizzato
    \end{itemize}
\end{enumerate}

\subsubsection{Fase 2: Content Analysis}

\begin{enumerate}
    \item \textbf{NLP Preprocessing}:
    \begin{itemize}
        \item Text cleaning e normalization (emoji, URL, hashtags)
        \item Tokenization con supporto per terminologia medica italiana
        \item POS tagging e dependency parsing con spaCy
        \item Language detection e handling contenuti multilingue
    \end{itemize}
    
    \item \textbf{Medical Entity Extraction}:
    \begin{itemize}
        \item Named Entity Recognition per entità mediche
        \item Identificazione farmaci, patologie, procedure
        \item Linking a ontologie mediche (UMLS, SNOMED-CT)
        \item Confidence scoring per ogni entità estratta
    \end{itemize}
    
    \item \textbf{Claim Detection e Classification}:
    \begin{itemize}
        \item Identificazione statement verificabili vs opinioni
        \item Classification per tipologia claim (efficacia, sicurezza, dosaggio)
        \item Extraction di claim principali e secondari
        \item Priority scoring per urgenza di fact-checking
    \end{itemize}
\end{enumerate}

\subsubsection{Fase 3: Evidence Retrieval}

\begin{enumerate}
    \item \textbf{Query Generation}:
    \begin{itemize}
        \item Automatic query construction da claim estratti
        \item Query expansion con sinonimi e termini correlati
        \item Multi-language query translation per fonti internazionali
        \item Query optimization per diversi database target
    \end{itemize}
    
    \item \textbf{Multi-Source Retrieval}:
    \begin{itemize}
        \item PubMed API integration per letteratura scientifica
        \item Institutional feeds (OMS, ISS, EMA, FDA, AIFA)
        \item Evergreen medical content database
        \item Real-time web scraping di fonti autorevoli
    \end{itemize}
    
    \item \textbf{Evidence Ranking e Selection}:
    \begin{itemize}
        \item Relevance scoring basato su semantic similarity
        \item Quality ranking (impact factor, peer review, recency)
        \item Evidence deduplication e clustering
        \item Selection di top-K evidenze per claim
    \end{itemize}
\end{enumerate}

\subsubsection{Fase 4: AI-Powered Fact-Checking}

\begin{enumerate}
    \item \textbf{Multi-LLM Orchestration}:
    \begin{itemize}
        \item Parallel processing con GPT-4, Claude-3, Gemini
        \item Load balancing automatico tra provider
        \item Fallback strategy su provider failure
        \item Cost optimization basata su complexity del claim
    \end{itemize}
    
    \item \textbf{Dual-Claim Analysis}:
    \begin{itemize}
        \item General claim evaluation (universally applicable)
        \item Local/temporal claim evaluation (context-specific)
        \item Cross-validation tra risultati LLM multipli
        \item Confidence aggregation con weighted voting
    \end{itemize}
    
    \item \textbf{Result Validation e Normalization}:
    \begin{itemize}
        \item Schema validation per output strutturato
        \item Consistency checking tra claim general/local
        \item Evidence citation validation
        \item Final scoring e verdict determination
    \end{itemize}
\end{enumerate}

\subsection{Post-Processing e Quality Assurance}

\subsubsection{Result Aggregation}

\begin{itemize}
    \item \textbf{Multi-dimensional Scoring}:
    \begin{itemize}
        \item Overall verdict (REAL/FAKE/UNCERTAIN)
        \item Confidence score (0.0-1.0)
        \item Disinformation grade (0-4 scale)
        \item Evidence quality assessment
    \end{itemize}
    
    \item \textbf{Explainability Generation}:
    \begin{itemize}
        \item Human-readable justification del verdict
        \item Citation linking a fonti scientifiche utilizzate
        \item Highlighting di portions critiche del testo
        \item Alternative interpretations per casi ambigui
    \end{itemize}
\end{itemize}

\subsubsection{Quality Control Pipeline}

\begin{itemize}
    \item \textbf{Consistency Validation}:
    \begin{itemize}
        \item Cross-check tra verdetti di LLM diversi
        \item Conflict resolution per risultati discordanti
        \item Threshold-based filtering per low-confidence results
        \item Escalation a human review per casi critici
    \end{itemize}
    
    \item \textbf{Continuous Learning}:
    \begin{itemize}
        \item Feedback integration da expert validation
        \item Model performance monitoring e drift detection
        \item Automatic retraining triggers
        \item A/B testing per model improvements
    \end{itemize}
\end{itemize}

\section{Metodologie di Sviluppo}

\subsection{Approccio di Sviluppo Software}

\subsubsection{Agile Development Framework}

\begin{itemize}
    \item \textbf{Sprint Organization}:
    \begin{itemize}
        \item Sprint di 2 settimane per development cycles
        \item Daily standups per coordination
        \item Sprint reviews con stakeholder feedback
        \item Retrospectives per continuous improvement
    \end{itemize}
    
    \item \textbf{User Story Driven Development}:
    \begin{itemize}
        \item Epic breakdown per major features
        \item User stories con acceptance criteria ben definiti
        \item Story point estimation per planning
        \item Velocity tracking per predictability
    \end{itemize}
    
    \item \textbf{Continuous Integration/Continuous Deployment}:
    \begin{itemize}
        \item GitHub Actions per automated CI/CD pipeline
        \item Automated testing ad ogni commit
        \item Staging environment per integration testing
        \item Blue-green deployment per zero-downtime releases
    \end{itemize}
\end{itemize}

\subsubsection{Quality Assurance Methodology}

\begin{itemize}
    \item \textbf{Test-Driven Development (TDD)}:
    \begin{itemize}
        \item Unit tests prima dell'implementazione
        \item Minimum 85\% code coverage requirement
        \item Integration tests per API endpoints
        \item End-to-end tests per user workflows critici
    \end{itemize}
    
    \item \textbf{Code Quality Standards}:
    \begin{itemize}
        \item Peer code reviews obbligatorie
        \item Static analysis con SonarQube
        \item Linting automatico (ESLint, Pylint)
        \item Security scanning con OWASP ZAP
    \end{itemize}
\end{itemize}

\subsection{Metodologie di Ricerca}

\subsubsection{Experimental Design}

\begin{itemize}
    \item \textbf{Controlled Experiments}:
    \begin{itemize}
        \item A/B testing per UI/UX improvements
        \item Randomized controlled trials per algorithm comparison
        \item Baseline establishment con existing solutions
        \item Statistical significance testing
    \end{itemize}
    
    \item \textbf{Dataset Development}:
    \begin{itemize}
        \item Ground truth creation con expert annotators medici
        \item Inter-annotator agreement measurement (Cohen's kappa > 0.8)
        \item Balanced dataset con diverse tipologie di disinformazione
        \item Continuous dataset expansion e refinement
    \end{itemize}
\end{itemize}

\subsubsection{Evaluation Methodology}

\begin{itemize}
    \item \textbf{Performance Metrics}:
    \begin{itemize}
        \item Classification accuracy (Precision, Recall, F1-score)
        \item Confusion matrix analysis per class-specific insights
        \item ROC-AUC per threshold optimization
        \item Calibration plots per confidence assessment
    \end{itemize}
    
    \item \textbf{Qualitative Assessment}:
    \begin{itemize}
        \item Expert evaluation da professionisti sanitari
        \item Usability studies con target users
        \item Case study analysis per edge cases
        \item Error analysis per systematic improvement
    \end{itemize}
\end{itemize}

\section{Metodologie di Validazione Scientifica}

\subsection{Validation Framework}

\subsubsection{Multi-Level Validation Approach}

\begin{itemize}
    \item \textbf{Technical Validation}:
    \begin{itemize}
        \item Unit testing per singoli componenti (>85\% coverage)
        \item Integration testing per workflow end-to-end
        \item Performance testing sotto load realistici
        \item Security testing per vulnerabilità comuni
    \end{itemize}
    
    \item \textbf{Algorithm Validation}:
    \begin{itemize}
        \item Cross-validation su dataset annotato da esperti
        \item Benchmarking contro soluzioni state-of-the-art
        \item Ablation studies per contribution dei componenti
        \item Robustness testing contro adversarial examples
    \end{itemize}
    
    \item \textbf{Domain Expert Validation}:
    \begin{itemize}
        \item Review da panel di medici e ricercatori
        \item Case studies su eventi reali di disinformazione
        \item Feedback qualitativo su usabilità e utilità
        \item Validation in real-world deployment scenarios
    \end{itemize}
\end{itemize}

\subsubsection{Continuous Monitoring e Improvement}

\begin{itemize}
    \item \textbf{Performance Monitoring}:
    \begin{itemize}
        \item Real-time metrics tracking (accuracy, latency, throughput)
        \item Model drift detection e alerting automatico
        \item User satisfaction surveys periodiche
        \item System health monitoring 24/7
    \end{itemize}
    
    \item \textbf{Feedback Integration}:
    \begin{itemize}
        \item User feedback collection tramite interfaccia
        \item Expert corrections integration nel training data
        \item Automated retraining basato su performance degradation
        \item Version control per model iterations
    \end{itemize}
\end{itemize}

\subsection{Ethical e Legal Compliance}

\subsubsection{Privacy e Data Protection}

\begin{itemize}
    \item \textbf{GDPR Compliance}:
    \begin{itemize}
        \item Data minimization principles
        \item User consent management
        \item Right to deletion implementation
        \item Data portability support
    \end{itemize}
    
    \item \textbf{Anonymization Strategy}:
    \begin{itemize}
        \item PII removal da contenuti processati
        \item Hashing di user identifiers
        \item Aggregation per statistical reporting
        \item Secure data storage e transmission
    \end{itemize}
\end{itemize}

\subsubsection{Ethical AI Guidelines}

\begin{itemize}
    \item \textbf{Transparency e Explainability}:
    \begin{itemize}
        \item Algoritmi interpretabili per decisioni critiche
        \item Documentation completa dei modelli utilizzati
        \item Justification automatic per ogni verdict
        \item Bias detection e mitigation strategies
    \end{itemize}
    
    \item \textbf{Fairness e Non-discrimination}:
    \begin{itemize}
        \item Testing per bias demografici
        \item Balanced representation nei training data
        \item Regular audit per algorithmic fairness
        \item Mitigation strategies per identified biases
    \end{itemize}
\end{itemize}

\section{Roadmap di Sviluppo}

\subsection{Fasi di Sviluppo}

\subsubsection{Fase 1: Foundation (Mesi 1-3)}

\begin{itemize}
    \item \textbf{Infrastructure Setup}:
    \begin{itemize}
        \item Setup ambiente di sviluppo e CI/CD pipeline
        \item Configurazione database e servizi core
        \item Implementazione autenticazione e security baseline
        \item Documentation e coding standards establishment
    \end{itemize}
    
    \item \textbf{Core Data Collection}:
    \begin{itemize}
        \item Implementazione base API collectors per Twitter e Reddit
        \item Sistema di storing e indexing in MongoDB
        \item Basic content filtering e deduplication
        \item Monitoring e logging infrastructure
    \end{itemize}
\end{itemize}

\subsubsection{Fase 2: NLP e Analysis Engine (Mesi 4-6)}

\begin{itemize}
    \item \textbf{NLP Pipeline Development}:
    \begin{itemize}
        \item Integration spaCy con modelli italiani
        \item Medical entity recognition implementation
        \item Claim detection e classification algorithms
        \item Sentiment analysis e polarization detection
    \end{itemize}
    
    \item \textbf{Evidence Retrieval System}:
    \begin{itemize}
        \item PubMed API integration
        \item Institutional feeds collector
        \item Semantic search implementation
        \item Evidence ranking e quality assessment
    \end{itemize}
\end{itemize}

\subsubsection{Fase 3: AI Fact-Checking Engine (Mesi 7-9)}

\begin{itemize}
    \item \textbf{Multi-LLM Integration}:
    \begin{itemize}
        \item OpenAI GPT-4 integration
        \item Anthropic Claude e Google Gemini integration
        \item Load balancing e fallback mechanisms
        \item Prompt engineering optimization
    \end{itemize}
    
    \item \textbf{Fact-Checking Logic}:
    \begin{itemize}
        \item Dual-claim analysis implementation
        \item Consensus mechanism development
        \item Result validation e normalization
        \item Explainability generation
    \end{itemize}
\end{itemize}

\subsubsection{Fase 4: User Interface e Campaign Management (Mesi 10-12)}

\begin{itemize}
    \item \textbf{Frontend Development}:
    \begin{itemize}
        \item React application con dashboard interattive
        \item Real-time updates e notifications
        \item Data visualization e reporting features
        \item Mobile-responsive design
    \end{itemize}
    
    \item \textbf{Campaign System}:
    \begin{itemize}
        \item User management e role-based access
        \item Campaign configuration interface
        \item Automated reporting e export capabilities
        \item API documentation e developer tools
    \end{itemize}
\end{itemize}

\subsection{Milestone e Deliverables}

\subsubsection{Technical Milestones}

\begin{itemize}
    \item \textbf{M1 (Mese 3)}: Basic data collection sistema operativo
    \item \textbf{M2 (Mese 6)}: NLP pipeline con accuratezza >80\% su entity extraction
    \item \textbf{M3 (Mese 9)}: AI fact-checking con accuratezza >85\% su dataset test
    \item \textbf{M4 (Mese 12)}: Sistema completo ready per beta testing
\end{itemize}

\subsubsection{Research Deliverables}

\begin{itemize}
    \item \textbf{Pubblicazioni Scientifiche}:
    \begin{itemize}
        \item Paper su metodologia dual-claim fact-checking
        \item Studio comparativo su performance multi-LLM
        \item Analysis di pattern di disinformazione medica italiana
    \end{itemize}
    
    \item \textbf{Dataset e Tools}:
    \begin{itemize}
        \item Annotated dataset di disinformazione medica italiana
        \item Open-source components per la community
        \item Benchmarking suite per fact-checking systems
    \end{itemize}
\end{itemize}

%=====================================
% ARCHITETTURA DEL SISTEMA
%=====================================
\chapter{Architettura del Sistema}

\section{Panoramica Architetturale}

\subsection{Principi di Design}

L'architettura del sistema è stata progettata seguendo i principi fondamentali del software engineering moderno:

\begin{itemize}
    \item \textbf{Separation of Concerns}: Netta separazione tra layer di presentazione, logica di business e persistenza dati
    \item \textbf{Modularità}: Componenti loosely coupled che possono essere sviluppati, testati e deployati indipendentemente
    \item \textbf{Scalabilità}: Progettazione per crescita orizzontale e gestione di carichi variabili
    \item \textbf{Resilienza}: Fault tolerance e graceful degradation in caso di failure di componenti
    \item \textbf{Osservabilità}: Logging, monitoring e tracing completi per debugging e ottimizzazione
\end{itemize}

\subsection{Architettura High-Level}

Il sistema adotta un'architettura a tre tier modificata con componenti specializzati:

\begin{figure}[H]
\centering
\begin{tikzpicture}[node distance=2cm, auto]
    % Frontend Layer
    \node[rectangle, draw, fill=blue!20, minimum width=8cm, minimum height=1.5cm] (frontend) {Frontend Layer (React SPA)};
    
    % API Gateway
    \node[rectangle, draw, fill=green!20, minimum width=8cm, minimum height=1cm, below=1cm of frontend] (gateway) {API Gateway \& Load Balancer};
    
    % Backend Services
    \node[rectangle, draw, fill=orange!20, minimum width=2.5cm, minimum height=1.2cm, below left=1.5cm and -3cm of gateway] (auth) {Auth\\Service};
    \node[rectangle, draw, fill=orange!20, minimum width=2.5cm, minimum height=1.2cm, below=1.5cm of gateway] (core) {Core API\\Services};
    \node[rectangle, draw, fill=orange!20, minimum width=2.5cm, minimum height=1.2cm, below right=1.5cm and 3cm of gateway] (ai) {AI/ML\\Engine};
    
    % Data Layer
    \node[rectangle, draw, fill=red!20, minimum width=2.2cm, minimum height=1cm, below left=1.5cm and -2.5cm of core] (mongo) {MongoDB};
    \node[rectangle, draw, fill=red!20, minimum width=2.2cm, minimum height=1cm, below=1.5cm of core] (elastic) {Elasticsearch};
    \node[rectangle, draw, fill=red!20, minimum width=2.2cm, minimum height=1cm, below right=1.5cm and 2.5cm of core] (redis) {Redis\\Cache};
    
    % External APIs
    \node[rectangle, draw, fill=yellow!20, minimum width=2cm, minimum height=0.8cm, right=2cm of ai] (social) {Social\\APIs};
    \node[rectangle, draw, fill=yellow!20, minimum width=2cm, minimum height=0.8cm, below=0.5cm of social] (pubmed) {PubMed\\API};
    \node[rectangle, draw, fill=yellow!20, minimum width=2cm, minimum height=0.8cm, below=0.5cm of pubmed] (llm) {LLM\\APIs};
    
    % Arrows
    \draw[->] (frontend) -- (gateway);
    \draw[->] (gateway) -- (auth);
    \draw[->] (gateway) -- (core);
    \draw[->] (gateway) -- (ai);
    
    \draw[->] (auth) -- (mongo);
    \draw[->] (core) -- (mongo);
    \draw[->] (core) -- (elastic);
    \draw[->] (core) -- (redis);
    \draw[->] (ai) -- (redis);
    
    \draw[->] (ai) -- (social);
    \draw[->] (ai) -- (pubmed);
    \draw[->] (ai) -- (llm);
    
\end{tikzpicture}
\caption{Architettura High-Level del Sistema}
\label{fig:high-level-arch}
\end{figure}

\subsection{Componenti Principali}

\textbf{1. Frontend Layer}
\begin{itemize}
    \item Single Page Application (SPA) sviluppata in React
    \item Gestione stato con Context API e hooks
    \item Routing client-side con React Router
    \item Interfaccia responsive con Tailwind CSS
\end{itemize}

\textbf{2. API Gateway}
\begin{itemize}
    \item Reverse proxy con bilanciamento del carico
    \item Rate limiting e throttling
    \item Gestione CORS e sicurezza
    \item Logging centralizzato delle richieste
\end{itemize}

\textbf{3. Backend Services}
\begin{itemize}
    \item Microservizi implementati in Flask
    \item Design pattern Repository per accesso dati
    \item Dependency Injection per testabilità
    \item Event-driven communication tra servizi
\end{itemize}

\textbf{4. Data Layer}
\begin{itemize}
    \item MongoDB per dati principali (document store)
    \item Elasticsearch per ricerca e analytics
    \item Redis per caching e session management
\end{itemize}

\section{Stack Tecnologico Dettagliato}

\subsection{Frontend Technologies}

\begin{table}[H]
\centering
\begin{tabular}{@{}lll@{}}
\toprule
\textbf{Tecnologia} & \textbf{Versione} & \textbf{Utilizzo} \\
\midrule
React & 19.1.1 & Framework UI principale \\
React Router DOM & 7.7.1 & Routing e navigation \\
Tailwind CSS & 3.4.4 & Styling e responsive design \\
Recharts & 3.1.0 & Visualizzazioni e grafici \\
React Force Graph & 1.48.0 & Network visualization \\
Lucide React & 0.535.0 & Iconografia \\
html2canvas & 1.4.1 & Export immagini \\
jsPDF & 3.0.1 & Export PDF \\
\bottomrule
\end{tabular}
\caption{Stack Tecnologico Frontend}
\label{tab:frontend-stack}
\end{table}

\subsection{Backend Technologies}

\begin{table}[H]
\centering
\begin{tabular}{@{}lll@{}}
\toprule
\textbf{Tecnologia} & \textbf{Versione} & \textbf{Utilizzo} \\
\midrule
Flask & 3.1.1 & Web framework Python \\
Flask-CORS & 6.0.1 & Cross-origin resource sharing \\
pymongo & 4.13.2 & MongoDB driver \\
elasticsearch & 9.1.0 & Search engine client \\
pandas & 2.3.1 & Data manipulation \\
scikit-learn & 1.7.1 & Machine learning \\
spaCy & 3.8.7 & NLP processing \\
sentence-transformers & 5.1.0 & Embeddings \\
\bottomrule
\end{tabular}
\caption{Stack Tecnologico Backend Core}
\label{tab:backend-stack}
\end{table}

\subsection{AI/ML Technologies}

\begin{table}[H]
\centering
\begin{tabular}{@{}lll@{}}
\toprule
\textbf{Tecnologia} & \textbf{Versione} & \textbf{Utilizzo} \\
\midrule
OpenAI API & 1.99.1 & GPT models integration \\
Anthropic API & 0.60.0 & Claude models integration \\
Google Generative AI & 0.8.5 & Gemini models integration \\
transformers & 4.54.1 & Hugging Face models \\
torch & 2.7.1 & Deep learning framework \\
nltk & 3.9.1 & Natural language toolkit \\
\bottomrule
\end{tabular}
\caption{Stack Tecnologico AI/ML}
\label{tab:ai-stack}
\end{table}

\subsection{Data Collection Technologies}

\begin{table}[H]
\centering
\begin{tabular}{@{}lll@{}}
\toprule
\textbf{Tecnologia} & \textbf{Versione} & \textbf{Utilizzo} \\
\midrule
tweepy & 4.16.0 & Twitter API client \\
praw & 7.8.1 & Reddit API client \\
selenium & 4.34.2 & Web automation \\
requests & 2.32.4 & HTTP client \\
beautifulsoup4 & 4.13.4 & HTML parsing \\
feedparser & 6.0.11 & RSS/Atom feeds \\
\bottomrule
\end{tabular}
\caption{Stack Tecnologico Data Collection}
\label{tab:datacollection-stack}
\end{table}

\section{Design Patterns Implementati}

\subsection{Repository Pattern}

Il sistema implementa il Repository Pattern per astrarre l'accesso ai dati:

\begin{lstlisting}[language=Python, caption=Esempio Repository Pattern]
class CampaignRepository:
    def __init__(self, db_manager):
        self.db = db_manager
        self.collection = db_manager.db.campaigns
    
    def create(self, campaign_data):
        return self.collection.insert_one(campaign_data)
    
    def find_by_user(self, user_id):
        return self.collection.find({"user_id": user_id})
    
    def update(self, campaign_id, update_data):
        return self.collection.update_one(
            {"_id": ObjectId(campaign_id)},
            {"$set": update_data}
        )
\end{lstlisting}

\subsection{Factory Pattern}

Utilizzato per la gestione dei servizi LLM:

\begin{lstlisting}[language=Python, caption=LLM Service Factory]
class LLMServiceFactory:
    @staticmethod
    def create_service(provider, model_name):
        if provider == "openai":
            return OpenAIService(model_name)
        elif provider == "claude":
            return ClaudeService(model_name)
        elif provider == "gemini":
            return GeminiService(model_name)
        else:
            raise ValueError(f"Unsupported provider: {provider}")
\end{lstlisting}

\subsection{Strategy Pattern}

Implementato per diverse strategie di data collection:

\begin{lstlisting}[language=Python, caption=Data Collection Strategy]
class DataCollectionStrategy:
    def collect(self, query, num_posts):
        raise NotImplementedError

class TwitterStrategy(DataCollectionStrategy):
    def collect(self, query, num_posts):
        # Twitter-specific collection logic
        pass

class RedditStrategy(DataCollectionStrategy):
    def collect(self, query, num_posts):
        # Reddit-specific collection logic
        pass
\end{lstlisting}

\subsection{Observer Pattern}

Utilizzato per la gestione degli eventi del sistema:

\begin{lstlisting}[language=Python, caption=Event System]
class EventManager:
    def __init__(self):
        self.listeners = {}
    
    def subscribe(self, event_type, listener):
        if event_type not in self.listeners:
            self.listeners[event_type] = []
        self.listeners[event_type].append(listener)
    
    def notify(self, event_type, data):
        for listener in self.listeners.get(event_type, []):
            listener.handle(data)
\end{lstlisting}

\section{Sicurezza e Autenticazione}

\subsection{Architettura di Sicurezza}

Il sistema implementa un modello di sicurezza a più livelli:

\begin{enumerate}
    \item \textbf{Transport Layer Security}: HTTPS/TLS 1.3 per tutte le comunicazioni
    \item \textbf{Authentication}: JWT tokens con refresh mechanism
    \item \textbf{Authorization}: Role-based access control (RBAC)
    \item \textbf{Input Validation}: Sanitizzazione e validazione di tutti gli input
    \item \textbf{Rate Limiting}: Protezione contro abuse e DoS attacks
\end{enumerate}

\subsection{JWT Token Management}

\begin{lstlisting}[language=Python, caption=JWT Token Management]
import jwt
from datetime import datetime, timedelta
from flask import current_app

class TokenManager:
    @staticmethod
    def generate_token(user_id, role):
        payload = {
            'user_id': user_id,
            'role': role,
            'exp': datetime.utcnow() + timedelta(hours=24),
            'iat': datetime.utcnow()
        }
        return jwt.encode(
            payload, 
            current_app.config['JWT_SECRET_KEY'], 
            algorithm='HS256'
        )
    
    @staticmethod
    def verify_token(token):
        try:
            payload = jwt.decode(
                token, 
                current_app.config['JWT_SECRET_KEY'], 
                algorithms=['HS256']
            )
            return payload
        except jwt.ExpiredSignatureError:
            return None
        except jwt.InvalidTokenError:
            return None
\end{lstlisting}

\subsection{Rate Limiting Implementation}

\begin{lstlisting}[language=Python, caption=Rate Limiting Decorator]
from functools import wraps
from flask import request, jsonify
import redis
import time

def rate_limit(max_requests=100, window=3600):
    def decorator(f):
        @wraps(f)
        def decorated_function(*args, **kwargs):
            client_ip = request.remote_addr
            key = f"rate_limit:{client_ip}"
            
            # Redis-based rate limiting
            r = redis.Redis()
            current_requests = r.get(key)
            
            if current_requests is None:
                r.setex(key, window, 1)
            else:
                if int(current_requests) >= max_requests:
                    return jsonify({
                        "error": "Rate limit exceeded"
                    }), 429
                r.incr(key)
            
            return f(*args, **kwargs)
        return decorated_function
    return decorator
\end{lstlisting}

\section{Scalabilità e Performance}

\subsection{Strategie di Caching}

Il sistema implementa caching multi-level:

\begin{itemize}
    \item \textbf{Application Level}: Caching in-memory per oggetti frequentemente acceduti
    \item \textbf{Database Level}: Query result caching con Redis
    \item \textbf{HTTP Level}: Response caching con appropriate cache headers
    \item \textbf{CDN Level}: Static assets delivery attraverso CDN
\end{itemize}

\subsection{Database Optimization}

\begin{lstlisting}[language=Python, caption=MongoDB Indexing Strategy]
# Indexes per performance optimization
db.social_posts.create_index([
    ("campaign_id", 1),
    ("created_utc", -1)
])

db.social_posts.create_index([
    ("analysis_results.grado_disinformazione", 1),
    ("campaign_id", 1)
])

# Compound index per query complesse
db.social_posts.create_index([
    ("campaign_id", 1),
    ("source", 1),
    ("analysis_results.sentiment", 1)
])

# Text index per full-text search
db.social_posts.create_index([
    ("text", "text"),
    ("title", "text")
])
\end{lstlisting}

\subsection{Asynchronous Processing}

Il sistema utilizza task asincroni per operazioni computazionalmente intensive:

\begin{lstlisting}[language=Python, caption=Celery Task Configuration]
from celery import Celery

# Celery configuration
celery_app = Celery(
    'medical_fake_news',
    broker='redis://localhost:6379/0',
    backend='redis://localhost:6379/0'
)

@celery_app.task
def analyze_post_async(post_id, campaign_id):
    """Analizza un post in modo asincrono"""
    try:
        # Recupera post dal database
        post = get_post_by_id(post_id)
        
        # Esegui analisi
        analysis_result = run_fact_check(post)
        
        # Salva risultati
        update_post_analysis(post_id, analysis_result)
        
        return {"status": "success", "post_id": post_id}
    except Exception as e:
        logger.error(f"Error analyzing post {post_id}: {e}")
        return {"status": "error", "error": str(e)}
\end{lstlisting}

%=====================================
% BACKEND - CORE SERVICES
%=====================================
\chapter{Backend - Core Services}

\section{Architettura del Backend}

\subsection{Flask Application Factory Pattern}

Il backend utilizza l'Application Factory Pattern per massimizzare la flessibilità e la testabilità:

\begin{lstlisting}[language=Python, caption=Flask Application Factory]
from flask import Flask
from flask_cors import CORS
from .core.config import Config
from .core.database.mongoDB import MongoDBManager

mongo_manager = MongoDBManager(Config.MONGO_URI, Config.DB_NAME)

def create_app():
    app = Flask(__name__)
    app.config.from_object(Config)
    
    # CORS configuration
    CORS(app, 
         origins=['http://localhost:3000', 'http://localhost:5173'],
         methods=['GET', 'POST', 'PUT', 'DELETE', 'OPTIONS'],
         allow_headers=['Content-Type', 'Authorization'],
         supports_credentials=True)

    register_blueprints(app)
    app.mongo_manager = mongo_manager
    return app

def register_blueprints(app):
    from .api.auth.routes import auth_bp
    from .api.campaigns.routes import campaigns_bp
    from .api.posts.routes import posts_bp
    from .api.data_collection.routes import data_collection_bp
    from .api.analysis.routes import analysis_bp
    from .api.authors.routes import authors_bp
    from .api.reports.routes import reports_bp
    
    app.register_blueprint(auth_bp, url_prefix='/api')
    app.register_blueprint(campaigns_bp, url_prefix='/api/campaigns')
    app.register_blueprint(posts_bp, url_prefix='/api/posts')
    app.register_blueprint(data_collection_bp, url_prefix='/api')
    app.register_blueprint(analysis_bp, url_prefix='/api/analysis')
    app.register_blueprint(authors_bp, url_prefix='/api')
    app.register_blueprint(reports_bp, url_prefix='/api')
\end{lstlisting}

\subsection{Database Management}

Il sistema utilizza MongoDB come database principale con un wrapper personalizzato per la gestione delle connessioni:

\begin{lstlisting}[language=Python, caption=MongoDB Manager]
from pymongo import MongoClient
import logging

class MongoDBManager:
    def __init__(self, uri, db_name):
        self.uri = uri
        self.db_name = db_name
        self.client = None
        self.db = None
        self._connect()
    
    def _connect(self):
        try:
            self.client = MongoClient(self.uri)
            self.db = self.client[self.db_name]
            # Test connection
            self.db.command('ping')
            logging.info(f"Connessione MongoDB stabilita: {self.db_name}")
        except Exception as e:
            logging.error(f"Errore connessione MongoDB: {e}")
            raise
    
    def get_collection(self, collection_name):
        return self.db[collection_name]
    
    def close_connection(self):
        if self.client:
            self.client.close()
\end{lstlisting}

\section{Sistema di Autenticazione}

\subsection{User Management Service}

Il servizio di autenticazione gestisce registrazione, login e gestione sessioni utente:

\begin{lstlisting}[language=Python, caption=Authentication Service]
import bcrypt
import jwt
from datetime import datetime, timedelta
from flask import current_app

class AuthService:
    @staticmethod
    def register_user(username, email, password):
        users_collection = current_app.mongo_manager.db.users
        
        # Check if user already exists
        if users_collection.find_one({"$or": [
            {"username": username}, 
            {"email": email}
        ]}):
            return {"message": "Utente già esistente"}, 400
        
        # Hash password
        password_hash = bcrypt.hashpw(
            password.encode('utf-8'), 
            bcrypt.gensalt()
        )
        
        # Create user document
        user_data = {
            "username": username,
            "email": email,
            "password_hash": password_hash,
            "role": "researcher",
            "created_at": datetime.now(),
            "last_login": None,
            "active": True
        }
        
        result = users_collection.insert_one(user_data)
        
        return {
            "message": "Utente registrato con successo",
            "user_id": str(result.inserted_id)
        }, 201
    
    @staticmethod
    def authenticate_user(username, password):
        users_collection = current_app.mongo_manager.db.users
        
        user = users_collection.find_one({
            "$or": [{"username": username}, {"email": username}]
        })
        
        if not user or not bcrypt.checkpw(
            password.encode('utf-8'), 
            user['password_hash']
        ):
            return {"message": "Credenziali non valide"}, 401
        
        # Update last login
        users_collection.update_one(
            {"_id": user["_id"]},
            {"$set": {"last_login": datetime.now()}}
        )
        
        # Generate JWT token
        token = AuthService.generate_token(
            str(user["_id"]), 
            user["role"]
        )
        
        return {
            "token": token,
            "user": {
                "id": str(user["_id"]),
                "username": user["username"],
                "email": user["email"],
                "role": user["role"]
            }
        }, 200
    
    @staticmethod
    def generate_token(user_id, role):
        payload = {
            'user_id': user_id,
            'role': role,
            'exp': datetime.utcnow() + timedelta(hours=24),
            'iat': datetime.utcnow()
        }
        return jwt.encode(
            payload, 
            current_app.config['JWT_SECRET_KEY'], 
            algorithm='HS256'
        )
\end{lstlisting}

\subsection{Authorization Decorators}

Sistema di decoratori per la gestione delle autorizzazioni:

\begin{lstlisting}[language=Python, caption=Authorization Decorators]
from functools import wraps
from flask import request, jsonify, current_app
import jwt

def token_required(f):
    @wraps(f)
    def decorated_function(*args, **kwargs):
        token = request.headers.get('Authorization')
        
        if not token:
            return jsonify({'message': 'Token mancante'}), 401
        
        try:
            if token.startswith('Bearer '):
                token = token[7:]
            
            payload = jwt.decode(
                token, 
                current_app.config['JWT_SECRET_KEY'], 
                algorithms=['HS256']
            )
            
            current_user_id = payload['user_id']
            current_user_role = payload['role']
            
        except jwt.ExpiredSignatureError:
            return jsonify({'message': 'Token scaduto'}), 401
        except jwt.InvalidTokenError:
            return jsonify({'message': 'Token non valido'}), 401
        
        return f(current_user_id, current_user_role, *args, **kwargs)
    
    return decorated_function

def role_required(required_role):
    def decorator(f):
        @wraps(f)
        @token_required
        def decorated_function(current_user_id, current_user_role, *args, **kwargs):
            if current_user_role != required_role:
                return jsonify({'message': 'Accesso non autorizzato'}), 403
            
            return f(current_user_id, current_user_role, *args, **kwargs)
        
        return decorated_function
    return decorator
\end{lstlisting}

\section{Campaign Management Service}

\subsection{Gestione delle Campagne}

Il servizio di gestione campagne è il core del sistema per l'organizzazione del monitoraggio:

\begin{lstlisting}[language=Python, caption=Campaign Service - Create Campaign]
from datetime import datetime, timedelta
from bson import ObjectId
from app.services.data_collection.data_collection_service import DataCollectionService

class CampaignService:
    @staticmethod
    def create_campaign(user_id, data):
        validation = validate_and_prepare_campaign_inputs(
            name=data.get("name"),
            keywords=data.get("keywords"),
            platforms=data.get("social_platforms")
        )
        
        if "errors" in validation:
            return {"status": "error", "errors": validation["errors"]}, 400
        
        name = validation["name"]
        keywords = validation["keywords"]
        platforms = validation["platforms"]

        try:
            campaign = {
                "user_id": user_id,
                "name": name,
                "start_date": datetime.now(),
                "social_platforms": platforms,
                "keywords": keywords,
                "status": "active",
                "created_at": datetime.now(),
                "last_updated": datetime.now(),
                "total_posts": 0,
                "analyzed_posts": 0,
                "fake_posts_count": 0
            }

            campaign_id = current_app.mongo_manager.db.campaigns.insert_one(campaign).inserted_id
            
            # Trigger initial data collection
            for keyword in keywords:
                for platform in platforms:
                    DataCollectionService.collect_posts_for_campaign(
                        query=keyword,
                        source=platform,
                        num_posts=50,
                        campaign_id=str(campaign_id),
                        mongo_manager=current_app.mongo_manager
                    )
            
            # Trigger analysis
            DataCollectionService.trigger_analysis_for_campaign(
                current_app.mongo_manager
            )

            return {
                "message": "Campagna creata con successo!",
                "campaign_id": str(campaign_id),
                "campaign": {
                    "name": name,
                    "start_date": campaign["start_date"].isoformat(),
                    "social_platforms": platforms,
                    "keywords": keywords
                }
            }, 201

        except Exception as e:
            logging.error(f"Errore creazione campagna: {e}", exc_info=True)
            return {"message": "Errore interno", "details": str(e)}, 500
\end{lstlisting}

\subsection{Campaign Validation}

Sistema di validazione per i dati delle campagne:

\begin{lstlisting}[language=Python, caption=Campaign Validation]
def validate_and_prepare_campaign_inputs(name, keywords, platforms):
    errors = []
    
    # Validate name
    if not name or not name.strip():
        errors.append("Nome campagna richiesto")
    elif len(name.strip()) < 3:
        errors.append("Nome campagna deve essere di almeno 3 caratteri")
    elif len(name.strip()) > 100:
        errors.append("Nome campagna non può superare i 100 caratteri")
    
    # Validate keywords
    if not keywords:
        errors.append("Almeno una parola chiave richiesta")
    else:
        if isinstance(keywords, str):
            keywords = [k.strip() for k in keywords.split(',') if k.strip()]
        
        if not keywords:
            errors.append("Almeno una parola chiave valida richiesta")
        elif len(keywords) > 20:
            errors.append("Massimo 20 parole chiave consentite")
        
        # Validate each keyword
        for keyword in keywords:
            if len(keyword) < 2:
                errors.append(f"Parola chiave '{keyword}' troppo corta")
            elif len(keyword) > 50:
                errors.append(f"Parola chiave '{keyword}' troppo lunga")
    
    # Validate platforms
    valid_platforms = {'twitter', 'facebook', 'reddit', 'youtube', 'instagram'}
    if not platforms:
        errors.append("Almeno una piattaforma social richiesta")
    else:
        invalid_platforms = set(platforms) - valid_platforms
        if invalid_platforms:
            errors.append(f"Piattaforme non supportate: {invalid_platforms}")
    
    if errors:
        return {"errors": errors}
    
    return {
        "name": name.strip(),
        "keywords": keywords,
        "platforms": platforms
    }
\end{lstlisting}

\section{Data Collection Service}

\subsection{Multi-Platform Data Collection}

Il sistema di raccolta dati supporta multiple piattaforme social:

\begin{lstlisting}[language=Python, caption=Data Collection Service]
import logging
from datetime import datetime
from app.services.data_collection.twitter_service import TwitterService
from app.services.data_collection.reddit_service import RedditService
from app.services.data_collection.youtube_service import YouTubeService

class DataCollectionService:
    @staticmethod
    def collect_posts_for_campaign(query, source, num_posts, campaign_id, mongo_manager):
        try:
            posts_data = []
            
            if source == 'twitter':
                twitter_service = TwitterService()
                posts_data = twitter_service.search_posts(query, num_posts)
            elif source == 'reddit':
                reddit_service = RedditService()
                posts_data = reddit_service.search_posts(query, num_posts)
            elif source == 'youtube':
                youtube_service = YouTubeService()
                posts_data = youtube_service.search_videos(query, num_posts)
            else:
                logging.warning(f"Piattaforma non supportata: {source}")
                return 0
            
            if not posts_data:
                return 0
            
            # Prepare posts for insertion
            posts_for_insertion = []
            for post in posts_data:
                post_doc = {
                    "campaign_id": campaign_id,
                    "source": source,
                    "query": query,
                    "title": post.get('title', ''),
                    "text": post.get('text', ''),
                    "author_name": post.get('author_name', 'Unknown'),
                    "url": post.get('url', ''),
                    "created_utc": post.get('created_utc', datetime.now()),
                    "score": post.get('score', 0),
                    "num_comments": post.get('num_comments', 0),
                    "collected_at": datetime.now(),
                    "analysis_results": None,
                    "stato_post": "da analizzare"
                }
                posts_for_insertion.append(post_doc)
            
            # Insert posts into database
            result = mongo_manager.db.social_posts.insert_many(posts_for_insertion)
            inserted_count = len(result.inserted_ids)
            
            # Update campaign statistics
            mongo_manager.db.campaigns.update_one(
                {"_id": ObjectId(campaign_id)},
                {
                    "$inc": {"total_posts": inserted_count},
                    "$set": {"last_updated": datetime.now()}
                }
            )
            
            logging.info(f"Inseriti {inserted_count} post per campagna {campaign_id}")
            return inserted_count
            
        except Exception as e:
            logging.error(f"Errore raccolta dati: {e}", exc_info=True)
            return 0
    
    @staticmethod
    def trigger_analysis_for_campaign(mongo_manager):
        try:
            # Find posts that need analysis
            posts_to_analyze = mongo_manager.db.social_posts.find({
                "analysis_results": None,
                "stato_post": "da analizzare"
            }).limit(100)
            
            from app.services.analysis.analysis_orchestrator import AnalysisOrchestrator
            orchestrator = AnalysisOrchestrator()
            
            analyzed_count = 0
            for post in posts_to_analyze:
                try:
                    # Run analysis
                    analysis_result = orchestrator.analyze_post(
                        post_text=post.get('text', ''),
                        post_title=post.get('title', ''),
                        keywords=post.get('query', '')
                    )
                    
                    # Update post with analysis results
                    mongo_manager.db.social_posts.update_one(
                        {"_id": post["_id"]},
                        {
                            "$set": {
                                "analysis_results": analysis_result,
                                "stato_post": "analizzato",
                                "analyzed_at": datetime.now()
                            }
                        }
                    )
                    
                    analyzed_count += 1
                    
                except Exception as post_error:
                    logging.error(f"Errore analisi post {post['_id']}: {post_error}")
                    continue
            
            logging.info(f"Analizzati {analyzed_count} post")
            return analyzed_count
            
        except Exception as e:
            logging.error(f"Errore trigger analisi: {e}", exc_info=True)
            return 0
\end{lstlisting}

\subsection{Platform-Specific Services}

\textbf{Twitter Service Implementation:}

\begin{lstlisting}[language=Python, caption=Twitter Service]
import tweepy
from app.core.config import Config

class TwitterService:
    def __init__(self):
        self.api = self._initialize_api()
    
    def _initialize_api(self):
        try:
            auth = tweepy.AppAuthHandler(
                Config.TWITTER_CONSUMER_KEY,
                Config.TWITTER_CONSUMER_SECRET
            )
            api = tweepy.API(auth, wait_on_rate_limit=True)
            return api
        except Exception as e:
            logging.error(f"Errore inizializzazione Twitter API: {e}")
            return None
    
    def search_posts(self, query, num_posts=50):
        if not self.api:
            return []
        
        posts = []
        try:
            # Add medical context to query
            medical_query = f"{query} medicina salute OR vaccine OR farmaco"
            
            tweets = tweepy.Cursor(
                self.api.search_tweets,
                q=medical_query,
                lang="it",
                result_type="recent",
                tweet_mode="extended"
            ).items(num_posts)
            
            for tweet in tweets:
                post_data = {
                    'title': '',
                    'text': tweet.full_text,
                    'author_name': tweet.user.screen_name,
                    'url': f"https://twitter.com/{tweet.user.screen_name}/status/{tweet.id}",
                    'created_utc': tweet.created_at,
                    'score': tweet.favorite_count,
                    'num_comments': tweet.reply_count or 0,
                    'platform_id': str(tweet.id)
                }
                posts.append(post_data)
            
        except Exception as e:
            logging.error(f"Errore ricerca Twitter: {e}")
        
        return posts
\end{lstlisting}

\textbf{Reddit Service Implementation:}

\begin{lstlisting}[language=Python, caption=Reddit Service]
import praw
from app.core.config import Config

class RedditService:
    def __init__(self):
        self.reddit = self._initialize_api()
    
    def _initialize_api(self):
        try:
            reddit = praw.Reddit(
                client_id=Config.REDDIT_CLIENT_ID,
                client_secret=Config.REDDIT_CLIENT_SECRET,
                user_agent=Config.REDDIT_USER_AGENT
            )
            return reddit
        except Exception as e:
            logging.error(f"Errore inizializzazione Reddit API: {e}")
            return None
    
    def search_posts(self, query, num_posts=50):
        if not self.reddit:
            return []
        
        posts = []
        try:
            # Search in medical subreddits
            subreddits = ['italy', 'ItaliaPersonalFinance', 'medicine', 'health']
            
            for subreddit_name in subreddits:
                subreddit = self.reddit.subreddit(subreddit_name)
                
                for submission in subreddit.search(query, limit=num_posts//len(subreddits)):
                    post_data = {
                        'title': submission.title,
                        'text': submission.selftext,
                        'author_name': str(submission.author) if submission.author else 'deleted',
                        'url': f"https://reddit.com{submission.permalink}",
                        'created_utc': datetime.fromtimestamp(submission.created_utc),
                        'score': submission.score,
                        'num_comments': submission.num_comments,
                        'platform_id': submission.id
                    }
                    posts.append(post_data)
            
        except Exception as e:
            logging.error(f"Errore ricerca Reddit: {e}")
        
        return posts
\end{lstlisting}

\section{Post Management Service}

\subsection{Post Retrieval and Filtering}

Sistema per il recupero e filtraggio dei post analizzati:

\begin{lstlisting}[language=Python, caption=Post Service]
from bson import ObjectId
from flask import current_app

class PostService:
    @staticmethod
    def get_posts_for_campaign(campaign_id, filters=None):
        try:
            # Build query
            query = {"campaign_id": campaign_id}
            
            if filters:
                # Filter by platform
                if filters.get('platforms'):
                    query['source'] = {'$in': filters['platforms']}
                
                # Filter by fake news score
                if filters.get('min_fake_score') is not None:
                    query['analysis_results.grado_disinformazione'] = {
                        '$gte': filters['min_fake_score']
                    }
                
                # Filter by date range
                if filters.get('start_date') or filters.get('end_date'):
                    date_filter = {}
                    if filters.get('start_date'):
                        date_filter['$gte'] = filters['start_date']
                    if filters.get('end_date'):
                        date_filter['$lte'] = filters['end_date']
                    query['created_utc'] = date_filter
            
            # Execute query with sorting
            posts = current_app.mongo_manager.db.social_posts.find(query).sort([
                ('analysis_results.grado_disinformazione', -1),
                ('created_utc', -1)
            ])
            
            posts_list = []
            for post in posts:
                post_data = {
                    "id": str(post["_id"]),
                    "platform": post.get("source", "N/A"),
                    "author": {
                        "name": post.get("author_name", "Unknown"),
                        "verified": False
                    },
                    "content": PostService._build_content(post),
                    "url": post.get("url"),
                    "score": post.get("score", 0),
                    "num_comments": post.get("num_comments", 0),
                    "created_at": post.get("created_utc", datetime.now()).isoformat(),
                    "keyword": post.get("query", "N/A"),
                    "analysis_results": post.get("analysis_results"),
                    "is_fake": PostService._determine_fake_status(post),
                    "sentiment": post.get("analysis_results", {}).get("sentiment", "neutral")
                }
                posts_list.append(post_data)
            
            return {"posts": posts_list}, 200
            
        except Exception as e:
            logging.error(f"Errore recupero post: {e}", exc_info=True)
            return {"message": "Errore interno"}, 500
    
    @staticmethod
    def _build_content(post):
        title = post.get('title', '')
        text = post.get('text', '')
        if title and text:
            return f"{title} - {text}"
        return title or text or "Contenuto non disponibile"
    
    @staticmethod
    def _determine_fake_status(post):
        analysis = post.get('analysis_results', {})
        grade = analysis.get('grado_disinformazione', 0)
        
        if grade >= 3:
            return "fake"
        elif grade >= 2:
            return "suspicious"
        elif grade >= 1:
            return "questionable"
        else:
            return "reliable"
    
    @staticmethod
    def get_post_details(post_id):
        try:
            post = current_app.mongo_manager.db.social_posts.find_one({
                "_id": ObjectId(post_id)
            })
            
            if not post:
                return {"message": "Post non trovato"}, 404
            
            # Enrich with additional analysis
            enriched_post = {
                "id": str(post["_id"]),
                "campaign_id": post.get("campaign_id"),
                "platform": post.get("source"),
                "content": PostService._build_content(post),
                "author_name": post.get("author_name"),
                "url": post.get("url"),
                "created_at": post.get("created_utc", datetime.now()).isoformat(),
                "analysis_results": post.get("analysis_results", {}),
                "medical_concepts": post.get("medical_concepts", []),
                "related_posts": PostService._get_related_posts(post),
                "fact_check_sources": PostService._extract_fact_check_sources(post)
            }
            
            return {"post": enriched_post}, 200
            
        except Exception as e:
            logging.error(f"Errore dettaglio post: {e}", exc_info=True)
            return {"message": "Errore interno"}, 500
    
    @staticmethod
    def _get_related_posts(post, limit=5):
        try:
            # Find posts with similar keywords or content
            campaign_id = post.get("campaign_id")
            query_keyword = post.get("query", "")
            
            related = current_app.mongo_manager.db.social_posts.find({
                "campaign_id": campaign_id,
                "query": query_keyword,
                "_id": {"$ne": post["_id"]}
            }).limit(limit)
            
            related_list = []
            for related_post in related:
                related_list.append({
                    "id": str(related_post["_id"]),
                    "content": PostService._build_content(related_post)[:100] + "...",
                    "platform": related_post.get("source"),
                    "fake_score": related_post.get("analysis_results", {}).get("grado_disinformazione", 0)
                })
            
            return related_list
            
        except Exception as e:
            logging.error(f"Errore post correlati: {e}")
            return []
    
    @staticmethod
    def _extract_fact_check_sources(post):
        analysis = post.get("analysis_results", {})
        sources = analysis.get("fonti_utilizzate", [])
        
        extracted_sources = []
        for source in sources:
            if isinstance(source, str) and source.startswith('http'):
                extracted_sources.append({
                    "url": source,
                    "title": "Fonte scientifica",
                    "type": "pubmed" if "pubmed" in source.lower() else "institutional"
                })
        
        return extracted_sources
\end{lstlisting}

%=====================================
% FRONTEND E USER INTERFACE
%=====================================
\chapter{Frontend e User Interface}

\section{Architettura Frontend}

\subsection{React Architecture Overview}

Il frontend è sviluppato come Single Page Application (SPA) utilizzando React con un'architettura moderna basata su hooks e context:

\begin{lstlisting}[language=JavaScript, caption=App.js - Main Application Structure]
import React from 'react';
import { Routes, Route } from 'react-router-dom';
import LoginPage from './pages/LoginPage';
import RegisterPage from './pages/RegisterPage';
import HomePage from './pages/HomePage';
import NotFoundPage from './pages/NotFoundPage';
import PrivateRoute from './components/PrivateRoute';
import ViewDetailsCampaign from './pages/ViewDetailsCampaign';
import PostDetails from './pages/PostDetails';
import AuthorDetails from './pages/AuthorDetails';
import ReportCampaign from './pages/ReportCampaign';

function App() {
  return (
    <Routes>
      <Route path="/login" element={<LoginPage />} />
      <Route path="/register" element={<RegisterPage />} />
      <Route path="*" element={<NotFoundPage />} />

      <Route element={<PrivateRoute />}>
        <Route path="/" element={<HomePage />} />
        <Route path="/campaigns/:campaignId" element={<ViewDetailsCampaign />}/>
        <Route path="/campaigns/:campaignId/post/:postId" element={<PostDetails />}/>
        <Route path="/campaigns/:campaignId/authors/:authorName" element={<AuthorDetails />} />
        <Route path="/campaigns/:campaignId/report" element={<ReportCampaign />} /> 
      </Route>
    </Routes>
  );
}

export default App;
\end{lstlisting}

\subsection{Authentication Context}

Sistema di gestione dell'autenticazione basato su React Context:

\begin{lstlisting}[language=JavaScript, caption=AuthContext.js - Authentication Management]
import React, { createContext, useState, useContext, useEffect } from 'react';

const AuthContext = createContext();

export const useAuth = () => {
  const context = useContext(AuthContext);
  if (!context) {
    throw new Error('useAuth must be used within an AuthProvider');
  }
  return context;
};

export const AuthProvider = ({ children }) => {
  const [user, setUser] = useState(null);
  const [token, setToken] = useState(localStorage.getItem('token'));
  const [loading, setLoading] = useState(true);

  useEffect(() => {
    const initializeAuth = async () => {
      const savedToken = localStorage.getItem('token');
      const savedUser = localStorage.getItem('user');

      if (savedToken && savedUser) {
        try {
          setToken(savedToken);
          setUser(JSON.parse(savedUser));
        } catch (error) {
          console.error('Error parsing saved user data:', error);
          logout();
        }
      }
      setLoading(false);
    };

    initializeAuth();
  }, []);

  const login = async (username, password) => {
    try {
      const response = await fetch('/api/login', {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json',
        },
        body: JSON.stringify({ username, password }),
      });

      if (response.ok) {
        const data = await response.json();
        setToken(data.token);
        setUser(data.user);
        localStorage.setItem('token', data.token);
        localStorage.setItem('user', JSON.stringify(data.user));
        return { success: true };
      } else {
        const errorData = await response.json();
        return { success: false, message: errorData.message };
      }
    } catch (error) {
      return { success: false, message: 'Errore di connessione' };
    }
  };

  const register = async (username, email, password) => {
    try {
      const response = await fetch('/api/register', {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json',
        },
        body: JSON.stringify({ username, email, password }),
      });

      if (response.ok) {
        return { success: true, message: 'Registrazione completata con successo' };
      } else {
        const errorData = await response.json();
        return { success: false, message: errorData.message };
      }
    } catch (error) {
      return { success: false, message: 'Errore di connessione' };
    }
  };

  const logout = () => {
    setUser(null);
    setToken(null);
    localStorage.removeItem('token');
    localStorage.removeItem('user');
  };

  const isAuthenticated = () => {
    return token !== null && user !== null;
  };

  const value = {
    user,
    token,
    login,
    register,
    logout,
    isAuthenticated,
    loading
  };

  return (
    <AuthContext.Provider value={value}>
      {children}
    </AuthContext.Provider>
  );
};
\end{lstlisting}

\section{Componenti Principali}

\subsection{Navigation Component}

Componente di navigazione principale con gestione responsive:

\begin{lstlisting}[language=JavaScript, caption=Navbar.jsx - Navigation Component]
import React, { useState } from 'react';
import { Link, useNavigate } from 'react-router-dom';
import { useAuth } from '../context/AuthContext';
import { Bars3Icon, XMarkIcon } from '@heroicons/react/24/outline';

const Navbar = () => {
  const { user, logout } = useAuth();
  const navigate = useNavigate();
  const [isMenuOpen, setIsMenuOpen] = useState(false);

  const handleLogout = () => {
    logout();
    navigate('/login');
  };

  const toggleMenu = () => {
    setIsMenuOpen(!isMenuOpen);
  };

  return (
    <nav className="bg-blue-600 text-white shadow-lg">
      <div className="max-w-7xl mx-auto px-4 sm:px-6 lg:px-8">
        <div className="flex justify-between h-16">
          {/* Logo and Brand */}
          <div className="flex items-center">
            <Link to="/" className="flex-shrink-0 flex items-center">
              <h1 className="text-xl font-bold">MedFakeNews Detector</h1>
            </Link>
          </div>

          {/* Desktop Navigation */}
          <div className="hidden md:flex items-center space-x-4">
            <Link 
              to="/" 
              className="hover:bg-blue-700 px-3 py-2 rounded-md text-sm font-medium transition-colors"
            >
              Dashboard
            </Link>
            
            <div className="flex items-center space-x-3">
              <span className="text-sm">
                Benvenuto, <span className="font-medium">{user?.username}</span>
              </span>
              <button
                onClick={handleLogout}
                className="bg-red-600 hover:bg-red-700 px-4 py-2 rounded-md text-sm font-medium transition-colors"
              >
                Logout
              </button>
            </div>
          </div>

          {/* Mobile menu button */}
          <div className="md:hidden flex items-center">
            <button
              onClick={toggleMenu}
              className="inline-flex items-center justify-center p-2 rounded-md hover:bg-blue-700 focus:outline-none focus:ring-2 focus:ring-inset focus:ring-white"
            >
              {isMenuOpen ? (
                <XMarkIcon className="block h-6 w-6" />
              ) : (
                <Bars3Icon className="block h-6 w-6" />
              )}
            </button>
          </div>
        </div>

        {/* Mobile Navigation */}
        {isMenuOpen && (
          <div className="md:hidden">
            <div className="px-2 pt-2 pb-3 space-y-1 sm:px-3">
              <Link
                to="/"
                className="hover:bg-blue-700 block px-3 py-2 rounded-md text-base font-medium"
                onClick={() => setIsMenuOpen(false)}
              >
                Dashboard
              </Link>
              <div className="border-t border-blue-500 pt-4">
                <div className="px-3 py-2">
                  <span className="text-sm text-blue-200">
                    Benvenuto, <span className="font-medium text-white">{user?.username}</span>
                  </span>
                </div>
                <button
                  onClick={() => {
                    handleLogout();
                    setIsMenuOpen(false);
                  }}
                  className="w-full text-left hover:bg-blue-700 block px-3 py-2 rounded-md text-base font-medium text-red-200 hover:text-red-100"
                >
                  Logout
                </button>
              </div>
            </div>
          </div>
        )}
      </div>
    </nav>
  );
};

export default Navbar;
\end{lstlisting}

\subsection{Campaign Creation Modal}

Componente modale per la creazione di nuove campagne:

\begin{lstlisting}[language=JavaScript, caption=CreateCampaignModal.jsx]
import React, { useState } from 'react';

const CreateCampaignModal = ({ isOpen, onClose, onCreate }) => {
  const [formData, setFormData] = useState({
    name: '',
    keywords: '',
    social_platforms: []
  });
  const [errors, setErrors] = useState({});
  const [loading, setLoading] = useState(false);

  const availablePlatforms = [
    { id: 'twitter', name: 'Twitter', description: 'Tweet e retweet' },
    { id: 'reddit', name: 'Reddit', description: 'Post e commenti da subreddit' },
    { id: 'youtube', name: 'YouTube', description: 'Video e commenti' },
    { id: 'facebook', name: 'Facebook', description: 'Post pubblici' }
  ];

  const validateForm = () => {
    const newErrors = {};

    if (!formData.name.trim()) {
      newErrors.name = 'Nome campagna richiesto';
    } else if (formData.name.trim().length < 3) {
      newErrors.name = 'Nome deve essere di almeno 3 caratteri';
    }

    if (!formData.keywords.trim()) {
      newErrors.keywords = 'Almeno una parola chiave richiesta';
    } else {
      const keywords = formData.keywords.split(',').map(k => k.trim()).filter(k => k);
      if (keywords.length === 0) {
        newErrors.keywords = 'Formato parole chiave non valido';
      } else if (keywords.length > 10) {
        newErrors.keywords = 'Massimo 10 parole chiave consentite';
      }
    }

    if (formData.social_platforms.length === 0) {
      newErrors.platforms = 'Seleziona almeno una piattaforma';
    }

    setErrors(newErrors);
    return Object.keys(newErrors).length === 0;
  };

  const handleSubmit = async (e) => {
    e.preventDefault();
    
    if (!validateForm()) return;

    setLoading(true);
    try {
      const campaignData = {
        name: formData.name.trim(),
        keywords: formData.keywords.split(',').map(k => k.trim()).filter(k => k),
        social_platforms: formData.social_platforms
      };

      await onCreate(campaignData);
      
      // Reset form
      setFormData({
        name: '',
        keywords: '',
        social_platforms: []
      });
      setErrors({});
      onClose();
    } catch (error) {
      setErrors({ submit: 'Errore durante la creazione della campagna' });
    } finally {
      setLoading(false);
    }
  };

  const handlePlatformChange = (platformId) => {
    setFormData(prev => ({
      ...prev,
      social_platforms: prev.social_platforms.includes(platformId)
        ? prev.social_platforms.filter(p => p !== platformId)
        : [...prev.social_platforms, platformId]
    }));
  };

  if (!isOpen) return null;

  return (
    <div className="fixed inset-0 bg-gray-600 bg-opacity-50 overflow-y-auto h-full w-full z-50 flex items-center justify-center">
      <div className="bg-white p-6 rounded-lg shadow-xl w-full max-w-md mx-4">
        <div className="flex justify-between items-center mb-4">
          <h3 className="text-xl font-bold text-gray-900">Crea Nuova Campagna</h3>
          <button
            onClick={onClose}
            className="text-gray-400 hover:text-gray-600 text-xl font-bold"
          >
            ×
          </button>
        </div>

        <form onSubmit={handleSubmit} className="space-y-4">
          {/* Campaign Name */}
          <div>
            <label className="block text-sm font-medium text-gray-700 mb-1">
              Nome Campagna *
            </label>
            <input
              type="text"
              value={formData.name}
              onChange={(e) => setFormData(prev => ({ ...prev, name: e.target.value }))}
              className={`w-full p-3 border rounded-md focus:ring-blue-500 focus:border-blue-500 ${
                errors.name ? 'border-red-300' : 'border-gray-300'
              }`}
              placeholder="Es: Monitoraggio Vaccini COVID-19"
              maxLength={100}
            />
            {errors.name && <p className="text-red-500 text-xs mt-1">{errors.name}</p>}
          </div>

          {/* Keywords */}
          <div>
            <label className="block text-sm font-medium text-gray-700 mb-1">
              Parole Chiave * <span className="text-gray-500">(separate da virgola)</span>
            </label>
            <textarea
              value={formData.keywords}
              onChange={(e) => setFormData(prev => ({ ...prev, keywords: e.target.value }))}
              className={`w-full p-3 border rounded-md focus:ring-blue-500 focus:border-blue-500 ${
                errors.keywords ? 'border-red-300' : 'border-gray-300'
              }`}
              placeholder="Es: vaccino, covid, pfizer, moderna, no-vax"
              rows={3}
            />
            {errors.keywords && <p className="text-red-500 text-xs mt-1">{errors.keywords}</p>}
          </div>

          {/* Social Platforms */}
          <div>
            <label className="block text-sm font-medium text-gray-700 mb-2">
              Piattaforme Social *
            </label>
            <div className="space-y-2">
              {availablePlatforms.map(platform => (
                <label key={platform.id} className="flex items-center">
                  <input
                    type="checkbox"
                    checked={formData.social_platforms.includes(platform.id)}
                    onChange={() => handlePlatformChange(platform.id)}
                    className="mr-3 h-4 w-4 text-blue-600 focus:ring-blue-500 border-gray-300 rounded"
                  />
                  <div>
                    <span className="font-medium">{platform.name}</span>
                    <p className="text-xs text-gray-500">{platform.description}</p>
                  </div>
                </label>
              ))}
            </div>
            {errors.platforms && <p className="text-red-500 text-xs mt-1">{errors.platforms}</p>}
          </div>

          {errors.submit && (
            <div className="bg-red-100 border border-red-400 text-red-700 px-4 py-3 rounded">
              {errors.submit}
            </div>
          )}

          <div className="flex justify-end space-x-3 pt-4">
            <button
              type="button"
              onClick={onClose}
              disabled={loading}
              className="px-4 py-2 text-gray-700 bg-gray-200 rounded-md hover:bg-gray-300 disabled:opacity-50"
            >
              Annulla
            </button>
            <button
              type="submit"
              disabled={loading}
              className="px-4 py-2 text-white bg-blue-600 rounded-md hover:bg-blue-700 disabled:opacity-50 flex items-center"
            >
              {loading ? (
                <>
                  <div className="animate-spin rounded-full h-4 w-4 border-b-2 border-white mr-2"></div>
                  Creazione...
                </>
              ) : (
                'Crea Campagna'
              )}
            </button>
          </div>
        </form>
      </div>
    </div>
  );
};

export default CreateCampaignModal;
\end{lstlisting}

\section{Data Visualization Components}

\subsection{Force Graph Network Visualization}

Componente per la visualizzazione delle reti di diffusione:

\begin{lstlisting}[language=JavaScript, caption=ForceGraphDisplay.jsx]
import React, { useEffect, useRef, useState } from 'react';
import ForceGraph2D from 'react-force-graph-2d';

const ForceGraphDisplay = ({ posts, authors }) => {
  const [graphData, setGraphData] = useState({ nodes: [], links: [] });
  const [selectedNode, setSelectedNode] = useState(null);
  const fgRef = useRef();

  useEffect(() => {
    if (!posts || !authors) return;

    const nodes = [];
    const links = [];
    
    // Create author nodes
    authors.forEach(author => {
      nodes.push({
        id: author.name,
        name: author.name,
        type: 'author',
        size: Math.max(5, Math.min(20, author.post_count)),
        color: author.avg_fake_score > 2 ? '#ef4444' : '#10b981',
        fake_score: author.avg_fake_score,
        post_count: author.post_count
      });
    });

    // Create post nodes and links
    posts.forEach(post => {
      const postId = `post_${post.id}`;
      
      nodes.push({
        id: postId,
        name: post.content.substring(0, 50) + '...',
        type: 'post',
        size: 3,
        color: post.is_fake === 'fake' ? '#dc2626' : 
               post.is_fake === 'suspicious' ? '#f59e0b' : '#16a34a',
        fake_score: post.analysis_results?.grado_disinformazione || 0,
        platform: post.platform
      });

      // Link post to author
      links.push({
        source: post.author.name,
        target: postId,
        strength: 1
      });
    });

    // Create author-to-author links based on shared topics
    const authorsByTopic = {};
    posts.forEach(post => {
      if (post.keyword) {
        if (!authorsByTopic[post.keyword]) {
          authorsByTopic[post.keyword] = [];
        }
        if (!authorsByTopic[post.keyword].includes(post.author.name)) {
          authorsByTopic[post.keyword].push(post.author.name);
        }
      }
    });

    // Create links between authors who post about the same topics
    Object.values(authorsByTopic).forEach(authorsInTopic => {
      for (let i = 0; i < authorsInTopic.length; i++) {
        for (let j = i + 1; j < authorsInTopic.length; j++) {
          links.push({
            source: authorsInTopic[i],
            target: authorsInTopic[j],
            strength: 0.3,
            type: 'topic_similarity'
          });
        }
      }
    });

    setGraphData({ nodes, links });
  }, [posts, authors]);

  const handleNodeClick = (node) => {
    setSelectedNode(node);
    
    // Highlight connected nodes
    if (fgRef.current) {
      const connectedNodes = new Set();
      graphData.links
        .filter(link => link.source.id === node.id || link.target.id === node.id)
        .forEach(link => {
          connectedNodes.add(link.source.id);
          connectedNodes.add(link.target.id);
        });
      
      fgRef.current.graphData({
        ...graphData,
        nodes: graphData.nodes.map(n => ({
          ...n,
          opacity: connectedNodes.has(n.id) || n.id === node.id ? 1 : 0.3
        }))
      });
    }
  };

  const nodeCanvasObject = (node, ctx, globalScale) => {
    const label = node.name;
    const fontSize = 12 / globalScale;
    ctx.font = `${fontSize}px Sans-Serif`;
    ctx.textAlign = 'center';
    ctx.textBaseline = 'middle';

    // Draw node
    ctx.fillStyle = node.color;
    ctx.beginPath();
    ctx.arc(node.x, node.y, node.size, 0, 2 * Math.PI, false);
    ctx.fill();

    // Draw label
    ctx.fillStyle = '#333';
    ctx.fillText(label, node.x, node.y + node.size + 2);
  };

  return (
    <div className="w-full h-96 border rounded-lg bg-gray-50">
      <div className="p-4 border-b bg-white">
        <h3 className="text-lg font-semibold">Network di Diffusione</h3>
        <div className="flex space-x-4 mt-2 text-sm">
          <div className="flex items-center">
            <div className="w-3 h-3 bg-green-500 rounded-full mr-2"></div>
            <span>Contenuto Affidabile</span>
          </div>
          <div className="flex items-center">
            <div className="w-3 h-3 bg-yellow-500 rounded-full mr-2"></div>
            <span>Contenuto Sospetto</span>
          </div>
          <div className="flex items-center">
            <div className="w-3 h-3 bg-red-500 rounded-full mr-2"></div>
            <span>Disinformazione</span>
          </div>
        </div>
      </div>

      <ForceGraph2D
        ref={fgRef}
        graphData={graphData}
        nodeLabel="name"
        nodeCanvasObject={nodeCanvasObject}
        onNodeClick={handleNodeClick}
        linkDirectionalArrowLength={3}
        linkDirectionalArrowRelPos={1}
        linkCurvature={0.25}
        enableNodeDrag={true}
        enableZoomInteraction={true}
        enablePanInteraction={true}
        width={800}
        height={300}
      />

      {selectedNode && (
        <div className="absolute top-4 right-4 bg-white p-4 rounded-lg shadow-lg border max-w-xs">
          <h4 className="font-semibold mb-2">{selectedNode.type === 'author' ? 'Autore' : 'Post'}</h4>
          <p className="text-sm text-gray-600 mb-1">
            <strong>Nome:</strong> {selectedNode.name}
          </p>
          {selectedNode.type === 'author' && (
            <>
              <p className="text-sm text-gray-600 mb-1">
                <strong>Post pubblicati:</strong> {selectedNode.post_count}
              </p>
              <p className="text-sm text-gray-600 mb-1">
                <strong>Score medio:</strong> {selectedNode.fake_score?.toFixed(1)}
              </p>
            </>
          )}
          {selectedNode.type === 'post' && (
            <>
              <p className="text-sm text-gray-600 mb-1">
                <strong>Piattaforma:</strong> {selectedNode.platform}
              </p>
              <p className="text-sm text-gray-600 mb-1">
                <strong>Score disinformazione:</strong> {selectedNode.fake_score}
              </p>
            </>
          )}
          <button
            onClick={() => setSelectedNode(null)}
            className="mt-2 text-xs text-blue-600 hover:text-blue-800"
          >
            Chiudi
          </button>
        </div>
      )}
    </div>
  );
};

export default ForceGraphDisplay;
\end{lstlisting}

\section{User Experience e Workflow}

\subsection{Campaign Dashboard}

Il dashboard principale fornisce una panoramica completa delle campagne attive:

\begin{table}[H]
\centering
\begin{tabular}{@{}ll@{}}
\toprule
\textbf{Elemento UI} & \textbf{Funzionalità} \\
\midrule
Campaign Cards & Visualizzazione stato e metriche campagne \\
Quick Actions & Creazione rapida, modifica, eliminazione \\
Filter Controls & Filtri per data, stato, piattaforma \\
Statistics Widget & KPI aggregati (post totali, fake rate, etc.) \\
Recent Activity & Timeline delle attività recenti \\
Search Bar & Ricerca rapida per nome campagna \\
\bottomrule
\end{tabular}
\caption{Elementi del Campaign Dashboard}
\label{tab:dashboard-elements}
\end{table}

\subsection{Post Analysis Interface}

L'interfaccia di analisi dei post presenta informazioni strutturate:

\begin{enumerate}
    \item \textbf{Post Overview}: Contenuto, autore, piattaforma, timestamp
    \item \textbf{AI Analysis Results}: Score di disinformazione, sentiment, confidenza
    \item \textbf{Evidence Sources}: Fonti utilizzate per la valutazione
    \item \textbf{Medical Concepts}: Entità mediche estratte automaticamente
    \item \textbf{Related Content}: Post correlati e pattern di diffusione
    \item \textbf{Action Buttons}: Esportazione, condivisione, flagging manuale
\end{enumerate}

\subsection{Responsive Design}

Il sistema implementa un design completamente responsive:

\begin{lstlisting}[language=CSS, caption=Responsive Design Classes (Tailwind)]
/* Mobile-first approach */
.container {
  @apply px-4 sm:px-6 lg:px-8 mx-auto max-w-7xl;
}

.campaign-grid {
  @apply grid grid-cols-1 md:grid-cols-2 lg:grid-cols-3 gap-6;
}

.stats-grid {
  @apply grid grid-cols-1 sm:grid-cols-2 lg:grid-cols-4 gap-4;
}

.mobile-menu {
  @apply block md:hidden;
}

.desktop-nav {
  @apply hidden md:flex md:items-center md:space-x-4;
}

/* Adaptive text sizes */
.title {
  @apply text-xl sm:text-2xl lg:text-3xl font-bold;
}

.card {
  @apply p-4 sm:p-6 bg-white rounded-lg shadow-md;
}

/* Interactive states */
.interactive-element {
  @apply transition-colors duration-200 hover:bg-gray-50 focus:outline-none focus:ring-2 focus:ring-blue-500;
}
\end{lstlisting}

%=====================================
% MODULI SPECIALIZZATI
%=====================================
\chapter{Moduli Specializzati}

\section{Sistema NLP e Analisi Linguistica}

\subsection{Text Processing Pipeline}

Il sistema NLP implementa una pipeline completa per l'analisi dei contenuti medici:

\begin{lstlisting}[language=Python, caption=NLP Processing Pipeline]
import spacy
import re
from app.nlp.preprocessing.text_cleaner import TextCleaner
from app.nlp.preprocessing.language_detector import LanguageDetector
from app.nlp.extraction.entity_extractor import EntityExtractor
from app.nlp.extraction.term_extractor import TermExtractor

class NLPPipeline:
    def __init__(self):
        # Load Italian spaCy model
        self.nlp = spacy.load("it_core_news_lg")
        
        # Initialize components
        self.text_cleaner = TextCleaner()
        self.language_detector = LanguageDetector()
        self.entity_extractor = EntityExtractor()
        self.term_extractor = TermExtractor()
        
        # Medical terminology patterns
        self.medical_patterns = self._load_medical_patterns()
    
    def process_text(self, text, extract_entities=True):
        """
        Processa un testo attraverso la pipeline NLP completa
        """
        results = {
            'original_text': text,
            'processed': {},
            'entities': {},
            'medical_concepts': [],
            'signals': {}
        }
        
        try:
            # 1. Language Detection
            detected_lang = self.language_detector.detect(text)
            results['language'] = detected_lang
            
            # 2. Text Cleaning and Preprocessing
            cleaned_text = self.text_cleaner.clean_text(text)
            results['processed']['cleaned_text'] = cleaned_text
            
            # 3. spaCy Processing
            doc = self.nlp(cleaned_text)
            results['processed']['tokens'] = len(doc)
            results['processed']['sentences'] = len(list(doc.sents))
            
            # 4. Entity Extraction
            if extract_entities:
                entities = self.entity_extractor.extract_entities(doc)
                results['entities'] = entities
                
                # Extract medical-specific entities
                medical_entities = self.entity_extractor.extract_medical_entities(doc)
                results['medical_concepts'] = medical_entities
            
            # 5. Term Extraction
            important_terms = self.term_extractor.extract_key_terms(doc)
            results['key_terms'] = important_terms
            
            # 6. Signal Extraction
            signals = self._extract_signals(doc, text)
            results['signals'] = signals
            
            return results
            
        except Exception as e:
            logging.error(f"Errore processing NLP: {e}")
            results['error'] = str(e)
            return results
    
    def _extract_signals(self, doc, original_text):
        """
        Estrae segnali specifici per il fact-checking medico
        """
        signals = {
            'certainty_markers': [],
            'negation_markers': [],
            'temporal_expressions': [],
            'geographic_expressions': [],
            'authority_claims': [],
            'emotional_language': [],
            'medical_jargon_complexity': 0
        }
        
        # Certainty markers (es: "sicuramente", "certamente", "senza dubbio")
        certainty_patterns = [
            r'\b(sicuramente|certamente|senza dubbio|è certo che|garantito)\b',
            r'\b(al 100%|completamente|totalmente|assolutamente)\b'
        ]
        
        for pattern in certainty_patterns:
            matches = re.finditer(pattern, original_text, re.IGNORECASE)
            for match in matches:
                signals['certainty_markers'].append({
                    'text': match.group(),
                    'start': match.start(),
                    'end': match.end()
                })
        
        # Negation markers
        negation_patterns = [
            r'\b(non|nessuno|niente|mai|nemmeno|neanche)\b',
            r'\b(impossibile|falso|sbagliato|errato)\b'
        ]
        
        for pattern in negation_patterns:
            matches = re.finditer(pattern, original_text, re.IGNORECASE)
            for match in matches:
                signals['negation_markers'].append({
                    'text': match.group(),
                    'start': match.start(),
                    'end': match.end()
                })
        
        # Extract temporal expressions
        for ent in doc.ents:
            if ent.label_ in ["DATE", "TIME"]:
                signals['temporal_expressions'].append({
                    'text': ent.text,
                    'label': ent.label_,
                    'start': ent.start_char,
                    'end': ent.end_char
                })
        
        # Extract geographic expressions
        for ent in doc.ents:
            if ent.label_ in ["GPE", "LOC"]:  # Geopolitical entity, Location
                signals['geographic_expressions'].append({
                    'text': ent.text,
                    'label': ent.label_,
                    'start': ent.start_char,
                    'end': ent.end_char
                })
        
        # Authority claims (es: "secondo gli esperti", "i medici dicono")
        authority_patterns = [
            r'\b(secondo (gli )?esperti|i medici (dicono|affermano))\b',
            r'\b(gli scienziati|i ricercatori|l\'OMS|il ministero)\b',
            r'\b(studi dimostrano|la ricerca conferma|è scientificamente provato)\b'
        ]
        
        for pattern in authority_patterns:
            matches = re.finditer(pattern, original_text, re.IGNORECASE)
            for match in matches:
                signals['authority_claims'].append({
                    'text': match.group(),
                    'start': match.start(),
                    'end': match.end()
                })
        
        # Emotional language
        emotional_patterns = [
            r'\b(shock|incredibile|scandaloso|terrificante|allarmante)\b',
            r'\b(miracoloso|straordinario|rivoluzionario|sensazionale)\b',
            r'\b(pericoloso|mortale|devastante|catastrofico)\b'
        ]
        
        for pattern in emotional_patterns:
            matches = re.finditer(pattern, original_text, re.IGNORECASE)
            for match in matches:
                signals['emotional_language'].append({
                    'text': match.group(),
                    'start': match.start(),
                    'end': match.end(),
                    'type': 'emotional'
                })
        
        # Medical jargon complexity score
        medical_terms_count = len(signals.get('medical_concepts', []))
        total_tokens = len(doc)
        signals['medical_jargon_complexity'] = medical_terms_count / total_tokens if total_tokens > 0 else 0
        
        return signals
    
    def _load_medical_patterns(self):
        """
        Carica pattern per terminologia medica italiana
        """
        return [
            # Farmaci e principi attivi
            r'\b\w*(farmaco|medicina|principio attivo|dosaggio|posologia)\w*\b',
            # Condizioni mediche
            r'\b\w*(malattia|patologia|sindrome|disturbo|infezione)\w*\b',
            # Procedure mediche
            r'\b\w*(terapia|trattamento|cura|intervento|diagnosi)\w*\b',
            # Anatomia
            r'\b\w*(cuore|polmoni|fegato|reni|cervello|sangue)\w*\b'
        ]
\end{lstlisting}

\subsection{Medical Entity Recognition}

Sistema specializzato per il riconoscimento di entità mediche:

\begin{lstlisting}[language=Python, caption=Medical Entity Extractor]
import re
import logging
from typing import List, Dict, Tuple

class MedicalEntityExtractor:
    def __init__(self):
        self.drug_patterns = self._load_drug_patterns()
        self.condition_patterns = self._load_condition_patterns()
        self.procedure_patterns = self._load_procedure_patterns()
        
        # Medical terminology dictionaries
        self.medical_dictionaries = {
            'drugs': self._load_drug_dictionary(),
            'conditions': self._load_condition_dictionary(),
            'procedures': self._load_procedure_dictionary(),
            'anatomy': self._load_anatomy_dictionary()
        }
    
    def extract_medical_entities(self, doc) -> Dict[str, List[Dict]]:
        """
        Estrae entità mediche specifiche dal testo processato con spaCy
        """
        entities = {
            'drugs': [],
            'medical_conditions': [],
            'procedures': [],
            'anatomy': [],
            'dosages': [],
            'temporal_medical': []
        }
        
        text = doc.text
        
        # Extract drug mentions
        entities['drugs'] = self._extract_drugs(text)
        
        # Extract medical conditions
        entities['medical_conditions'] = self._extract_conditions(text)
        
        # Extract medical procedures
        entities['procedures'] = self._extract_procedures(text)
        
        # Extract anatomical references
        entities['anatomy'] = self._extract_anatomy(text)
        
        # Extract dosage information
        entities['dosages'] = self._extract_dosages(text)
        
        # Extract temporal medical expressions
        entities['temporal_medical'] = self._extract_temporal_medical(text)
        
        return entities
    
    def _extract_drugs(self, text: str) -> List[Dict]:
        """Estrae menzioni di farmaci e principi attivi"""
        drugs = []
        
        # Pattern per farmaci comuni
        drug_patterns = [
            r'\b(paracetamolo|ibuprofene|aspirina|amoxicillina|cortisone)\b',
            r'\b(pfizer|moderna|astrazeneca|johnson|johnson & johnson)\b',
            r'\b(vaccino|vaccini|siero|immunizzazione)\b',
            r'\b(antibiotico|antivirale|antinfiammatorio|analgesico)\b'
        ]
        
        for pattern in drug_patterns:
            matches = re.finditer(pattern, text, re.IGNORECASE)
            for match in matches:
                drugs.append({
                    'text': match.group(),
                    'start': match.start(),
                    'end': match.end(),
                    'type': 'drug',
                    'confidence': 0.8
                })
        
        # Dictionary-based extraction
        for drug_name in self.medical_dictionaries['drugs']:
            if drug_name.lower() in text.lower():
                start = text.lower().find(drug_name.lower())
                drugs.append({
                    'text': drug_name,
                    'start': start,
                    'end': start + len(drug_name),
                    'type': 'drug',
                    'confidence': 0.9,
                    'source': 'dictionary'
                })
        
        return self._deduplicate_entities(drugs)
    
    def _extract_conditions(self, text: str) -> List[Dict]:
        """Estrae menzioni di condizioni mediche"""
        conditions = []
        
        condition_patterns = [
            r'\b(covid-19|coronavirus|sars-cov-2|omicron|delta)\b',
            r'\b(influenza|raffreddore|bronchite|polmonite|asma)\b',
            r'\b(diabete|ipertensione|colesterolo|obesità)\b',
            r'\b(cancro|tumore|neoplasia|metastasi|oncologia)\b',
            r'\b(depressione|ansia|disturbo bipolare|schizofrenia)\b',
            r'\b(artrite|artrosi|reumatismi|fibromialgia)\b'
        ]
        
        for pattern in condition_patterns:
            matches = re.finditer(pattern, text, re.IGNORECASE)
            for match in matches:
                conditions.append({
                    'text': match.group(),
                    'start': match.start(),
                    'end': match.end(),
                    'type': 'medical_condition',
                    'confidence': 0.85
                })
        
        return self._deduplicate_entities(conditions)
    
    def _extract_procedures(self, text: str) -> List[Dict]:
        """Estrae menzioni di procedure mediche"""
        procedures = []
        
        procedure_patterns = [
            r'\b(intervento chirurgico|operazione|biopsia|trapianto)\b',
            r'\b(risonanza magnetica|tac|ecografia|radiografia|endoscopia)\b',
            r'\b(chemioterapia|radioterapia|immunoterapia)\b',
            r'\b(vaccinazione|iniezione|infusione|trasfusione)\b',
            r'\b(fisioterapia|riabilitazione|terapia)\b'
        ]
        
        for pattern in procedure_patterns:
            matches = re.finditer(pattern, text, re.IGNORECASE)
            for match in matches:
                procedures.append({
                    'text': match.group(),
                    'start': match.start(),
                    'end': match.end(),
                    'type': 'medical_procedure',
                    'confidence': 0.8
                })
        
        return self._deduplicate_entities(procedures)
    
    def _extract_dosages(self, text: str) -> List[Dict]:
        """Estrae informazioni su dosaggi e posologie"""
        dosages = []
        
        dosage_patterns = [
            r'\b\d+\s*(mg|gr|g|ml|mcg|ui|iu)\b',
            r'\b\d+\s*(volte al giorno|al dì|ogni \d+ ore)\b',
            r'\b(una|due|tre|quattro|cinque)\s+(volta|volte)\s+(al giorno|al dì)\b',
            r'\b(mattina|sera|pranzo|cena|prima dei pasti|dopo i pasti)\b'
        ]
        
        for pattern in dosage_patterns:
            matches = re.finditer(pattern, text, re.IGNORECASE)
            for match in matches:
                dosages.append({
                    'text': match.group(),
                    'start': match.start(),
                    'end': match.end(),
                    'type': 'dosage',
                    'confidence': 0.9
                })
        
        return dosages
    
    def _deduplicate_entities(self, entities: List[Dict]) -> List[Dict]:
        """Rimuove entità duplicate o sovrapposte"""
        if not entities:
            return entities
        
        # Sort by start position
        entities.sort(key=lambda x: x['start'])
        
        deduplicated = []
        for entity in entities:
            # Check for overlap with existing entities
            overlap = False
            for existing in deduplicated:
                if (entity['start'] >= existing['start'] and 
                    entity['start'] < existing['end']) or \
                   (entity['end'] > existing['start'] and 
                    entity['end'] <= existing['end']):
                    overlap = True
                    break
            
            if not overlap:
                deduplicated.append(entity)
        
        return deduplicated
    
    def _load_drug_dictionary(self) -> List[str]:
        """Carica dizionario farmaci italiani comuni"""
        return [
            "paracetamolo", "ibuprofene", "aspirina", "diclofenac", "naprossene",
            "amoxicillina", "ciprofloxacina", "azitromicina", "claritromicina",
            "omeprazolo", "lansoprazolo", "ranitidina", "simvastatina",
            "atorvastatina", "metformina", "insulina", "warfarin", "eparina",
            "prednisone", "cortisone", "desametasone", "levotiroxina"
        ]
    
    def _load_condition_dictionary(self) -> List[str]:
        """Carica dizionario condizioni mediche comuni"""
        return [
            "covid-19", "coronavirus", "influenza", "polmonite", "bronchite",
            "asma", "diabete", "ipertensione", "ipotensione", "tachicardia",
            "aritmia", "infarto", "ictus", "embolia", "trombosi",
            "artrite", "artrosi", "osteoporosi", "fibromialgia",
            "depressione", "ansia", "bipolare", "alzheimer", "parkinson"
        ]
\end{lstlisting}

\section{Integrazione Servizi AI}

\subsection{LLM Manager per Multi-Provider Support}

Sistema unificato per gestire multiple API di Large Language Models:

\begin{lstlisting}[language=Python, caption=LLM Manager Implementation]
import logging
from typing import Dict, List, Optional, Any
from app.core.config import Config
from app.services.analysis.prompt_builder import PromptBuilder
from app.services.analysis.result_validator import ResultValidator

class LLMManager:
    def __init__(self):
        self.llm_services = {}
        self.failed_llms = set()
        self.logger = logging.getLogger(__name__)
        
        self.prompt_builder = PromptBuilder()
        self.result_validator = ResultValidator()
        
        # Initialize available services
        self._initialize_llm_services()
        self.service_index = 0
    
    def _initialize_llm_services(self):
        """Inizializza tutti i servizi LLM disponibili"""
        
        # OpenAI GPT
        if Config.OPENAI_API_KEY:
            try:
                from .providers.openai_service import OpenAIService
                openai_service = OpenAIService(
                    model_name=Config.OPENAI_MODEL or "gpt-4"
                )
                if openai_service.is_available:
                    self.llm_services['openai'] = openai_service
                    self.logger.info("OpenAI service initialized")
            except Exception as e:
                self.logger.error(f"Error initializing OpenAI: {e}")
        
        # Anthropic Claude
        if Config.CLAUDE_API_KEY:
            try:
                from .providers.claude_service import ClaudeService
                claude_service = ClaudeService(
                    model_name=Config.CLAUDE_MODEL or "claude-3-sonnet-20240229"
                )
                if claude_service.is_available:
                    self.llm_services['claude'] = claude_service
                    self.logger.info("Claude service initialized")
            except Exception as e:
                self.logger.error(f"Error initializing Claude: {e}")
        
        # Google Gemini
        if Config.GEMINI_API_KEY:
            try:
                from .providers.gemini_service import GeminiService
                gemini_service = GeminiService(
                    model_name=Config.GEMINI_MODEL or "gemini-1.5-pro"
                )
                if gemini_service.is_available:
                    self.llm_services['gemini'] = gemini_service
                    self.logger.info("Gemini service initialized")
            except Exception as e:
                self.logger.error(f"Error initializing Gemini: {e}")
        
        if not self.llm_services:
            self.logger.error("No LLM services initialized successfully!")
    
    def factcheck_with_retry(self, post_text: str, evidence_chunks: List[dict], 
                           max_retries: int = 3) -> Optional[Dict]:
        """
        Esegue fact-checking with automatic retry across different LLM providers
        """
        attempts = 0
        available_services = [name for name in self.llm_services.keys() 
                            if name not in self.failed_llms]
        
        while attempts < max_retries and available_services:
            service_name = available_services[attempts % len(available_services)]
            service = self.llm_services[service_name]
            
            try:
                self.logger.info(f"Attempting fact-check with {service_name} (attempt {attempts + 1})")
                
                # Build specialized prompt
                prompt = self.prompt_builder.build_factcheck_prompt(
                    post_text=post_text,
                    evidence_chunks=evidence_chunks
                )
                
                # Execute fact-check
                result = service.factcheck_with_evidence(
                    post_text=post_text,
                    evidence_chunks=evidence_chunks,
                    prompt=prompt
                )
                
                # Validate result
                if self.result_validator.validate_factcheck_result(result):
                    self.logger.info(f"Successful fact-check with {service_name}")
                    return result
                else:
                    self.logger.warning(f"Invalid result from {service_name}")
                    attempts += 1
                    continue
                    
            except Exception as e:
                self.logger.error(f"Error with {service_name}: {e}")
                self.mark_as_failed(service_name)
                available_services.remove(service_name)
                attempts += 1
                continue
        
        self.logger.error("All fact-check attempts failed")
        return None
    
    def analyze_sentiment(self, text: str) -> str:
        """Analizza sentiment del testo"""
        for service_name, service in self.llm_services.items():
            if service_name in self.failed_llms:
                continue
                
            try:
                result = service.analyze_sentiment_only(text)
                if result:
                    return result
            except Exception as e:
                self.logger.warning(f"Sentiment analysis failed with {service_name}: {e}")
                continue
        
        return "neutral"
    
    def get_next_service(self):
        """Ottiene il prossimo servizio disponibile in round-robin"""
        available_services = [
            service for name, service in self.llm_services.items()
            if name not in self.failed_llms
        ]
        
        if not available_services:
            return None
        
        service = available_services[self.service_index % len(available_services)]
        self.service_index = (self.service_index + 1) % len(available_services)
        return service
    
    def mark_as_failed(self, service_name: str):
        """Marca un servizio come fallito"""
        self.failed_llms.add(service_name)
        self.logger.warning(f"Service {service_name} marked as failed")
    
    def reset_failed_services(self):
        """Reset dello stato dei servizi falliti"""
        self.failed_llms.clear()
        self.logger.info("Failed services reset")
    
    def get_service_status(self) -> Dict[str, str]:
        """Ottiene lo stato di tutti i servizi"""
        status = {}
        for name in self.llm_services.keys():
            if name in self.failed_llms:
                status[name] = "failed"
            else:
                status[name] = "available"
        return status
\end{lstlisting}

\subsection{Provider-Specific Implementations}

\textbf{OpenAI Service Implementation:}

\begin{lstlisting}[language=Python, caption=OpenAI Service Provider]
import openai
import json
import logging
from typing import Dict, List, Optional

class OpenAIService:
    def __init__(self, model_name="gpt-4"):
        self.model_name = model_name
        self.client = None
        self.is_available = False
        self.logger = logging.getLogger(__name__)
        
        self._initialize_client()
    
    def _initialize_client(self):
        """Inizializza il client OpenAI"""
        try:
            from openai import OpenAI
            self.client = OpenAI(api_key=Config.OPENAI_API_KEY)
            
            # Test connection
            response = self.client.models.list()
            self.is_available = True
            self.logger.info(f"OpenAI client initialized with model {self.model_name}")
            
        except Exception as e:
            self.logger.error(f"Failed to initialize OpenAI client: {e}")
            self.is_available = False
    
    def factcheck_with_evidence(self, post_text: str, evidence_chunks: List[dict], 
                              prompt: str) -> Optional[Dict]:
        """
        Esegue fact-checking utilizzando GPT con evidenze fornite
        """
        if not self.is_available:
            return None
        
        try:
            # Prepare evidence context
            evidence_text = ""
            for i, chunk in enumerate(evidence_chunks[:10], 1):
                content = chunk.get('content', '')[:500]  # Limit length
                url = chunk.get('meta', {}).get('url', 'N/A')
                evidence_text += f"[Evidenza {i}]: {content}\nFonte: {url}\n\n"
            
            # Build comprehensive prompt
            system_prompt = """
            Sei un esperto fact-checker medico. Analizza il post fornito utilizzando le evidenze scientifiche.
            Devi fornire una valutazione strutturata in JSON con il seguente formato:
            
            {
                "general_claim": {
                    "verdict": "REAL|FAKE|UNCERTAIN",
                    "reasoning": "Spiegazione della valutazione generale",
                    "cited_evidence": [{"idx": 1, "title": "...", "url": "..."}]
                },
                "local_claim": {
                    "verdict": "REAL|FAKE|UNCERTAIN", 
                    "reasoning": "Spiegazione per aspetti locali/temporali specifici",
                    "cited_evidence": [{"idx": 1, "title": "...", "url": "..."}]
                },
                "overall_verdict": "REAL|FAKE|UNCERTAIN",
                "confidence": 0.0-1.0
            }
            """
            
            user_prompt = f"""
            POST DA ANALIZZARE:
            {post_text}
            
            EVIDENZE SCIENTIFICHE DISPONIBILI:
            {evidence_text}
            
            Analizza il post considerando:
            1. Accuratezza delle affermazioni mediche
            2. Presenza di claim locali/temporali specifici
            3. Qualità e credibilità delle fonti citate nel post
            4. Coerenza con le evidenze scientifiche fornite
            
            Fornisci la valutazione in formato JSON valido.
            """
            
            response = self.client.chat.completions.create(
                model=self.model_name,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ],
                temperature=0.3,
                max_tokens=1500,
                response_format={"type": "json_object"}
            )
            
            result_text = response.choices[0].message.content
            result = json.loads(result_text)
            
            # Validate and enrich result
            return self._validate_and_enrich_result(result)
            
        except Exception as e:
            self.logger.error(f"OpenAI fact-check error: {e}")
            return None
    
    def analyze_sentiment_only(self, text: str) -> str:
        """Analizza solo il sentiment del testo"""
        if not self.is_available:
            return "neutral"
        
        try:
            response = self.client.chat.completions.create(
                model=self.model_name,
                messages=[
                    {
                        "role": "system",
                        "content": "Analizza il sentiment del testo medico. Rispondi solo con: positivo, negativo, o neutro."
                    },
                    {
                        "role": "user", 
                        "content": f"Testo: {text[:1000]}"
                    }
                ],
                temperature=0.1,
                max_tokens=10
            )
            
            sentiment = response.choices[0].message.content.strip().lower()
            
            # Normalize sentiment
            if sentiment in ["positivo", "positive"]:
                return "positive"
            elif sentiment in ["negativo", "negative"]:
                return "negative"
            else:
                return "neutral"
                
        except Exception as e:
            self.logger.error(f"Sentiment analysis error: {e}")
            return "neutral"
    
    def _validate_and_enrich_result(self, result: Dict) -> Dict:
        """Valida e arricchisce il risultato del fact-checking"""
        
        # Ensure required fields
        required_fields = ["general_claim", "local_claim", "overall_verdict", "confidence"]
        for field in required_fields:
            if field not in result:
                self.logger.warning(f"Missing field in OpenAI result: {field}")
                return None
        
        # Validate verdict values
        valid_verdicts = ["REAL", "FAKE", "UNCERTAIN"]
        if result["overall_verdict"] not in valid_verdicts:
            result["overall_verdict"] = "UNCERTAIN"
        
        # Ensure confidence is in valid range
        confidence = result.get("confidence", 0.5)
        if not isinstance(confidence, (int, float)) or not 0 <= confidence <= 1:
            result["confidence"] = 0.5
        
        # Add metadata
        result["_metadata"] = {
            "provider": "openai",
            "model": self.model_name,
            "timestamp": datetime.now().isoformat()
        }
        
        return result
\end{lstlisting}

\section{Sistema PubMed e Validazione Scientifica}

\subsection{PubMed Integration Service}

Integrazione con il database PubMed per la validazione scientifica:

\begin{lstlisting}[language=Python, caption=PubMed Service Implementation]
from Bio import Entrez
import requests
import logging
from typing import List, Dict, Optional
from datetime import datetime, timedelta

class PubMedService:
    def __init__(self, email: str, api_key: str = None):
        self.email = email
        self.api_key = api_key
        self.logger = logging.getLogger(__name__)
        
        # Configure Entrez
        Entrez.email = email
        if api_key:
            Entrez.api_key = api_key
        
        self.base_url = "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/"
        self.max_results = 20
    
    def search_medical_literature(self, query: str, max_results: int = None) -> List[Dict]:
        """
        Cerca letteratura medica su PubMed per un dato query
        """
        if max_results is None:
            max_results = self.max_results
        
        try:
            # Build enhanced query with medical filters
            enhanced_query = self._enhance_medical_query(query)
            
            # Search PubMed
            handle = Entrez.esearch(
                db="pubmed",
                term=enhanced_query,
                retmax=max_results,
                sort="relevance",
                retmode="xml"
            )
            
            search_results = Entrez.read(handle)
            handle.close()
            
            if not search_results["IdList"]:
                self.logger.info(f"No PubMed results found for query: {query}")
                return []
            
            # Fetch detailed information
            pmids = search_results["IdList"]
            articles = self._fetch_article_details(pmids)
            
            # Process and rank results
            processed_articles = self._process_articles(articles, query)
            
            return processed_articles
            
        except Exception as e:
            self.logger.error(f"PubMed search error: {e}")
            return []
    
    def _enhance_medical_query(self, query: str) -> str:
        """
        Migliora il query aggiungendo filtri medici appropriati
        """
        # Add medical subject headings and publication type filters
        enhanced = f"({query})"
        
        # Add recent publication filter (last 5 years)
        current_year = datetime.now().year
        date_filter = f"AND ({current_year-5}[PDAT]:{current_year}[PDAT])"
        
        # Add quality filters
        quality_filters = "AND (systematic review[pt] OR meta analysis[pt] OR randomized controlled trial[pt] OR clinical trial[pt] OR review[pt])"
        
        # Add language filter
        language_filter = "AND (english[lang] OR italian[lang])"
        
        enhanced_query = enhanced + date_filter + quality_filters + language_filter
        
        return enhanced_query
    
    def _fetch_article_details(self, pmids: List[str]) -> List[Dict]:
        """
        Recupera i dettagli degli articoli da PubMed
        """
        try:
            # Fetch article summaries
            handle = Entrez.esummary(
                db="pubmed",
                id=",".join(pmids),
                retmode="xml"
            )
            summaries = Entrez.read(handle)
            handle.close()
            
            articles = []
            for summary in summaries:
                try:
                    # Extract key information
                    article = {
                        'pmid': summary.get('Id', ''),
                        'title': summary.get('Title', ''),
                        'authors': self._extract_authors(summary.get('Authors', [])),
                        'journal': summary.get('FullJournalName', summary.get('Source', '')),
                        'pub_date': summary.get('PubDate', ''),
                        'doi': summary.get('DOI', ''),
                        'abstract': '',  # Will be fetched separately if needed
                        'url': f"https://pubmed.ncbi.nlm.nih.gov/{summary.get('Id', '')}",
                        'publication_types': summary.get('PubTypeList', []),
                        'mesh_terms': []  # Medical Subject Headings
                    }
                    articles.append(article)
                    
                except Exception as e:
                    self.logger.warning(f"Error processing article {summary.get('Id', 'unknown')}: {e}")
                    continue
            
            # Fetch abstracts for top articles
            if articles:
                articles = self._fetch_abstracts(articles[:10])
            
            return articles
            
        except Exception as e:
            self.logger.error(f"Error fetching article details: {e}")
            return []
    
    def _fetch_abstracts(self, articles: List[Dict]) -> List[Dict]:
        """
        Recupera gli abstract degli articoli
        """
        try:
            pmids = [article['pmid'] for article in articles if article['pmid']]
            
            if not pmids:
                return articles
            
            handle = Entrez.efetch(
                db="pubmed",
                id=",".join(pmids),
                rettype="abstract",
                retmode="xml"
            )
            
            abstracts_data = Entrez.read(handle)
            handle.close()
            
            # Map abstracts to articles
            pmid_to_abstract = {}
            
            for record in abstracts_data['PubmedArticle']:
                try:
                    pmid = record['MedlineCitation']['PMID']
                    abstract_sections = record['MedlineCitation'].get('Article', {}).get('Abstract', {}).get('AbstractText', [])
                    
                    if abstract_sections:
                        if isinstance(abstract_sections, list):
                            abstract = " ".join([str(section) for section in abstract_sections])
                        else:
                            abstract = str(abstract_sections)
                        
                        pmid_to_abstract[str(pmid)] = abstract
                        
                except Exception as e:
                    self.logger.warning(f"Error extracting abstract: {e}")
                    continue
            
            # Update articles with abstracts
            for article in articles:
                article['abstract'] = pmid_to_abstract.get(article['pmid'], '')
            
            return articles
            
        except Exception as e:
            self.logger.error(f"Error fetching abstracts: {e}")
            return articles
    
    def _process_articles(self, articles: List[Dict], original_query: str) -> List[Dict]:
        """
        Processa e rankinza gli articoli in base alla rilevanza
        """
        processed = []
        
        for article in articles:
            try:
                # Calculate relevance score
                relevance_score = self._calculate_relevance(article, original_query)
                
                # Extract key concepts
                key_concepts = self._extract_key_concepts(article)
                
                # Assess article quality
                quality_score = self._assess_article_quality(article)
                
                processed_article = {
                    **article,
                    'relevance_score': relevance_score,
                    'quality_score': quality_score,
                    'key_concepts': key_concepts,
                    'evidence_strength': self._determine_evidence_strength(article),
                    'content_summary': self._create_content_summary(article)
                }
                
                processed.append(processed_article)
                
            except Exception as e:
                self.logger.warning(f"Error processing article {article.get('pmid', 'unknown')}: {e}")
                continue
        
        # Sort by combined score
        processed.sort(
            key=lambda x: (x['relevance_score'] * 0.6 + x['quality_score'] * 0.4), 
            reverse=True
        )
        
        return processed
    
    def _calculate_relevance(self, article: Dict, query: str) -> float:
        """
        Calcola la rilevanza dell'articolo rispetto al query
        """
        score = 0.0
        query_terms = query.lower().split()
        
        # Check title relevance
        title = article.get('title', '').lower()
        title_matches = sum(1 for term in query_terms if term in title)
        score += (title_matches / len(query_terms)) * 0.4
        
        # Check abstract relevance  
        abstract = article.get('abstract', '').lower()
        abstract_matches = sum(1 for term in query_terms if term in abstract)
        score += (abstract_matches / len(query_terms)) * 0.3
        
        # Bonus for systematic reviews and meta-analyses
        pub_types = [pt.lower() for pt in article.get('publication_types', [])]
        if any('systematic review' in pt or 'meta-analysis' in pt for pt in pub_types):
            score += 0.2
        
        # Bonus for recent publications
        pub_date = article.get('pub_date', '')
        if pub_date:
            try:
                # Simple year extraction
                year = int(pub_date.split()[-1])
                current_year = datetime.now().year
                if year >= current_year - 2:
                    score += 0.1
            except:
                pass
        
        return min(score, 1.0)
    
    def _assess_article_quality(self, article: Dict) -> float:
        """
        Valuta la qualità dell'articolo
        """
        score = 0.5  # Base score
        
        # Publication type scoring
        pub_types = [pt.lower() for pt in article.get('publication_types', [])]
        
        if any('systematic review' in pt for pt in pub_types):
            score += 0.3
        elif any('meta-analysis' in pt for pt in pub_types):
            score += 0.3
        elif any('randomized controlled trial' in pt for pt in pub_types):
            score += 0.2
        elif any('clinical trial' in pt for pt in pub_types):
            score += 0.1
        
        # Journal quality (simplified)
        journal = article.get('journal', '').lower()
        high_impact_journals = [
            'nature', 'science', 'cell', 'lancet', 'new england journal',
            'jama', 'bmj', 'plos medicine', 'cochrane'
        ]
        
        if any(journal_name in journal for journal_name in high_impact_journals):
            score += 0.2
        
        return min(score, 1.0)
    
    def _extract_authors(self, authors_data) -> List[str]:
        """
        Estrae i nomi degli autori
        """
        authors = []
        try:
            if isinstance(authors_data, list):
                for author in authors_data[:5]:  # Limit to first 5 authors
                    if isinstance(author, dict):
                        name = author.get('Name', str(author))
                    else:
                        name = str(author)
                    authors.append(name)
        except:
            pass
        
        return authors
    
    def validate_medical_claims(self, claims: List[str]) -> Dict[str, Dict]:
        """
        Valida una lista di affermazioni mediche usando PubMed
        """
        validation_results = {}
        
        for claim in claims:
            try:
                # Search for supporting evidence
                articles = self.search_medical_literature(claim, max_results=5)
                
                if articles:
                    # Analyze evidence strength
                    supporting_evidence = len([a for a in articles if a['relevance_score'] > 0.7])
                    total_evidence = len(articles)
                    
                    validation_results[claim] = {
                        'evidence_found': total_evidence > 0,
                        'supporting_articles': supporting_evidence,
                        'total_articles': total_evidence,
                        'confidence': supporting_evidence / max(total_evidence, 1),
                        'top_sources': articles[:3]
                    }
                else:
                    validation_results[claim] = {
                        'evidence_found': False,
                        'supporting_articles': 0,
                        'total_articles': 0,
                        'confidence': 0.0,
                        'top_sources': []
                    }
                    
            except Exception as e:
                self.logger.error(f"Error validating claim '{claim}': {e}")
                validation_results[claim] = {
                    'error': str(e),
                    'evidence_found': False,
                    'confidence': 0.0
                }
        
        return validation_results
\end{lstlisting}

%=====================================
% CONFIGURAZIONE E DEPLOYMENT
%=====================================
\chapter{Configurazione e Deployment}

\section{Requisiti di Sistema}

\subsection{Requisiti Hardware}

Il sistema è progettato per essere scalabile, ma i requisiti minimi raccomandati sono:

\textbf{Server di Produzione:}
\begin{itemize}
    \item CPU: 8 cores (Intel Xeon o AMD EPYC)
    \item RAM: 16 GB (32 GB raccomandati per carichi elevati)
    \item Storage: 500 GB SSD (per database e cache)
    \item Rete: 1 Gbps di banda
\end{itemize}

\textbf{Server di Sviluppo:}
\begin{itemize}
    \item CPU: 4 cores
    \item RAM: 8 GB
    \item Storage: 100 GB SSD
    \item Rete: 100 Mbps
\end{itemize}

\subsection{Requisiti Software}

\textbf{Sistema Operativo:}
\begin{itemize}
    \item Ubuntu 20.04 LTS o superiore
    \item CentOS 8 o superiore
    \item Docker con Docker Compose (raccomandato)
\end{itemize}

\textbf{Runtime e Dipendenze:}
\begin{itemize}
    \item Python 3.9+
    \item Node.js 18+ e npm
    \item MongoDB 5.0+
    \item Elasticsearch 8.0+
    \item Redis 6.0+
    \item Nginx (come reverse proxy)
\end{itemize}

\section{Configurazione Environment}

\subsection{Variabili di Ambiente}

Il sistema utilizza un file `.env` per la configurazione:

\begin{lstlisting}[language=bash, caption=Esempio file .env]
# Database Configuration
MONGO_URI=mongodb://localhost:27017/
DB_NAME=medical_fake_news_db

# JWT Configuration
JWT_SECRET_KEY=your-super-secret-jwt-key-here

# Social Media APIs
YOUTUBE_API_KEY=your-youtube-api-key
TWITTER_BEARER_TOKEN=your-twitter-bearer-token
TWITTER_EMAIL=your-twitter-email
TWITTER_PASSWORD=your-twitter-password

FACEBOOK_EMAIL=your-facebook-email
FACEBOOK_PASSWORD=your-facebook-password

REDDIT_CLIENT_ID=your-reddit-client-id
REDDIT_CLIENT_SECRET=your-reddit-client-secret
REDDIT_USER_AGENT=medical_fake_news_detector_v1

# LLM APIs
GEMINI_API_KEY=your-gemini-api-key
GEMINI_MODEL=gemini-1.5-pro
CLAUDE_API_KEY=your-claude-api-key
CLAUDE_MODEL=claude-3-sonnet-20240229
OPENAI_API_KEY=your-openai-api-key
OPENAI_MODEL=gpt-4

# Elasticsearch
ES_HOST=localhost
ES_PORT=9200
ES_CLOUD_ID=your-elastic-cloud-id
ES_API_KEY=your-elastic-api-key

# PubMed
PUBMED_EMAIL=your-email@domain.com
ENTREZ_API_KEY=your-entrez-api-key

# MetaMap (Optional)
METAMAP_UTS_API_KEY=your-metamap-api-key
METAMAP_UTS_USERNAME=your-metamap-username
METAMAP_UTS_PASSWORD=your-metamap-password
\end{lstlisting}

\subsection{Configurazione Database}

\textbf{MongoDB Setup:}

\begin{lstlisting}[language=bash, caption=MongoDB Configuration]
# Install MongoDB
sudo apt-get update
sudo apt-get install -y mongodb

# Start MongoDB service
sudo systemctl start mongod
sudo systemctl enable mongod

# Create database and user
mongo <<EOF
use medical_fake_news_db
db.createUser({
  user: "app_user",
  pwd: "secure_password",
  roles: [
    { role: "readWrite", db: "medical_fake_news_db" }
  ]
})
EOF

# Create indexes for performance
mongo medical_fake_news_db <<EOF
db.social_posts.createIndex({ "campaign_id": 1, "created_utc": -1 })
db.social_posts.createIndex({ "analysis_results.grado_disinformazione": 1 })
db.social_posts.createIndex({ "source": 1, "campaign_id": 1 })
db.campaigns.createIndex({ "user_id": 1, "status": 1 })
db.users.createIndex({ "username": 1 }, { unique: true })
db.users.createIndex({ "email": 1 }, { unique: true })
EOF
\end{lstlisting}

\textbf{Elasticsearch Setup:}

\begin{lstlisting}[language=bash, caption=Elasticsearch Configuration]
# Install Elasticsearch
wget -qO - https://artifacts.elastic.co/GPG-KEY-elasticsearch | sudo apt-key add -
echo "deb https://artifacts.elastic.co/packages/8.x/apt stable main" | sudo tee /etc/apt/sources.list.d/elastic-8.x.list
sudo apt-get update && sudo apt-get install elasticsearch

# Configure Elasticsearch
sudo nano /etc/elasticsearch/elasticsearch.yml

# Basic configuration
cluster.name: medical-fake-news-cluster
node.name: node-1
path.data: /var/lib/elasticsearch
path.logs: /var/log/elasticsearch
network.host: localhost
http.port: 9200
discovery.type: single-node

# Start Elasticsearch
sudo systemctl start elasticsearch
sudo systemctl enable elasticsearch

# Create index templates
curl -X PUT "localhost:9200/_index_template/posts_template" -H 'Content-Type: application/json' -d'
{
  "index_patterns": ["posts-*"],
  "template": {
    "settings": {
      "number_of_shards": 1,
      "number_of_replicas": 0,
      "analysis": {
        "analyzer": {
          "italian_analyzer": {
            "type": "standard",
            "stopwords": "_italian_"
          }
        }
      }
    },
    "mappings": {
      "properties": {
        "content": {
          "type": "text",
          "analyzer": "italian_analyzer",
          "fields": {
            "keyword": {
              "type": "keyword",
              "ignore_above": 256
            }
          }
        },
        "created_at": {
          "type": "date"
        },
        "platform": {
          "type": "keyword"
        },
        "fake_score": {
          "type": "integer"
        }
      }
    }
  }
}'
\end{lstlisting}

\section{Deployment con Docker}

\subsection{Docker Compose Configuration}

\begin{lstlisting}[language=yaml, caption=docker-compose.yml]
version: '3.8'

services:
  # Backend API
  backend:
    build:
      context: ./backend
      dockerfile: Dockerfile
    ports:
      - "5000:5000"
    environment:
      - FLASK_ENV=production
      - MONGO_URI=mongodb://mongo:27017/
      - REDIS_URL=redis://redis:6379
      - ES_HOST=elasticsearch
    depends_on:
      - mongo
      - redis
      - elasticsearch
    volumes:
      - ./backend:/app
      - ./logs:/app/logs
    restart: unless-stopped

  # Frontend
  frontend:
    build:
      context: ./frontend
      dockerfile: Dockerfile
    ports:
      - "80:80"
    depends_on:
      - backend
    restart: unless-stopped

  # MongoDB
  mongo:
    image: mongo:5.0
    ports:
      - "27017:27017"
    environment:
      - MONGO_INITDB_DATABASE=medical_fake_news_db
    volumes:
      - mongo_data:/data/db
      - ./mongo-init.js:/docker-entrypoint-initdb.d/mongo-init.js:ro
    restart: unless-stopped

  # Redis
  redis:
    image: redis:6.2-alpine
    ports:
      - "6379:6379"
    volumes:
      - redis_data:/data
    restart: unless-stopped

  # Elasticsearch
  elasticsearch:
    image: docker.elastic.co/elasticsearch/elasticsearch:8.5.0
    environment:
      - discovery.type=single-node
      - "ES_JAVA_OPTS=-Xms512m -Xmx512m"
      - xpack.security.enabled=false
    ports:
      - "9200:9200"
    volumes:
      - es_data:/usr/share/elasticsearch/data
    restart: unless-stopped

  # Nginx Reverse Proxy
  nginx:
    image: nginx:alpine
    ports:
      - "443:443"
    volumes:
      - ./nginx.conf:/etc/nginx/nginx.conf
      - ./ssl:/etc/nginx/ssl
    depends_on:
      - frontend
      - backend
    restart: unless-stopped

volumes:
  mongo_data:
  redis_data:
  es_data:
\end{lstlisting}

\subsection{Dockerfile per Backend}

\begin{lstlisting}[language=dockerfile, caption=backend/Dockerfile]
FROM python:3.9-slim

# Set working directory
WORKDIR /app

# Install system dependencies
RUN apt-get update && apt-get install -y \
    build-essential \
    curl \
    software-properties-common \
    git \
    && rm -rf /var/lib/apt/lists/*

# Install Python dependencies
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# Download spaCy model
RUN python -m spacy download it_core_news_lg

# Copy application code
COPY . .

# Create logs directory
RUN mkdir -p logs

# Expose port
EXPOSE 5000

# Health check
HEALTHCHECK --interval=30s --timeout=10s --start-period=5s --retries=3 \
    CMD curl -f http://localhost:5000/api/health || exit 1

# Run application
CMD ["python", "run.py"]
\end{lstlisting}

\subsection{Dockerfile per Frontend}

\begin{lstlisting}[language=dockerfile, caption=frontend/Dockerfile]
# Build stage
FROM node:18-alpine AS build

WORKDIR /app

# Copy package files
COPY package*.json ./

# Install dependencies
RUN npm ci --only=production

# Copy source code
COPY . .

# Build application
RUN npm run build

# Production stage
FROM nginx:alpine

# Copy built application
COPY --from=build /app/build /usr/share/nginx/html

# Copy nginx configuration
COPY nginx.conf /etc/nginx/conf.d/default.conf

# Expose port
EXPOSE 80

# Health check
HEALTHCHECK --interval=30s --timeout=3s --start-period=5s --retries=3 \
    CMD wget --no-verbose --tries=1 --spider http://localhost/ || exit 1

CMD ["nginx", "-g", "daemon off;"]
\end{lstlisting}

\section{Configurazione Nginx}

\begin{lstlisting}[language=nginx, caption=nginx.conf]
upstream backend {
    server backend:5000;
}

upstream frontend {
    server frontend:80;
}

# Rate limiting
limit_req_zone $binary_remote_addr zone=api:10m rate=10r/s;
limit_req_zone $binary_remote_addr zone=general:10m rate=100r/s;

server {
    listen 443 ssl http2;
    server_name your-domain.com;

    # SSL Configuration
    ssl_certificate /etc/nginx/ssl/cert.pem;
    ssl_certificate_key /etc/nginx/ssl/key.pem;
    ssl_protocols TLSv1.2 TLSv1.3;
    ssl_ciphers ECDHE-RSA-AES128-GCM-SHA256:ECDHE-RSA-AES256-GCM-SHA384;
    ssl_prefer_server_ciphers off;

    # Security headers
    add_header X-Frame-Options DENY;
    add_header X-Content-Type-Options nosniff;
    add_header X-XSS-Protection "1; mode=block";
    add_header Strict-Transport-Security "max-age=63072000; includeSubDomains; preload";

    # Gzip compression
    gzip on;
    gzip_vary on;
    gzip_min_length 1024;
    gzip_types
        text/plain
        text/css
        text/xml
        text/javascript
        application/javascript
        application/xml+rss
        application/json;

    # API routes
    location /api/ {
        limit_req zone=api burst=20 nodelay;
        
        proxy_pass http://backend;
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_set_header X-Forwarded-Proto $scheme;
        
        # Timeout settings
        proxy_connect_timeout 60s;
        proxy_send_timeout 60s;
        proxy_read_timeout 60s;
    }

    # Frontend routes
    location / {
        limit_req zone=general burst=50 nodelay;
        
        proxy_pass http://frontend;
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_set_header X-Forwarded-Proto $scheme;
        
        # Handle React Router
        try_files $uri $uri/ /index.html;
    }

    # Static assets caching
    location ~* \.(js|css|png|jpg|jpeg|gif|ico|svg)$ {
        expires 1y;
        add_header Cache-Control "public, immutable";
    }
}

# Redirect HTTP to HTTPS
server {
    listen 80;
    server_name your-domain.com;
    return 301 https://$server_name$request_uri;
}
\end{lstlisting}

\section{Monitoraggio e Logging}

\subsection{Configurazione Logging}

\begin{lstlisting}[language=python, caption=Logging Configuration]
import logging
import logging.handlers
import os
from datetime import datetime

def setup_logging():
    """Configura il sistema di logging per l'applicazione"""
    
    # Create logs directory
    logs_dir = "logs"
    os.makedirs(logs_dir, exist_ok=True)
    
    # Configure formatters
    detailed_formatter = logging.Formatter(
        '%(asctime)s - %(name)s - %(levelname)s - %(funcName)s:%(lineno)d - %(message)s'
    )
    
    simple_formatter = logging.Formatter(
        '%(asctime)s - %(levelname)s - %(message)s'
    )
    
    # Root logger
    root_logger = logging.getLogger()
    root_logger.setLevel(logging.INFO)
    
    # Console handler
    console_handler = logging.StreamHandler()
    console_handler.setLevel(logging.INFO)
    console_handler.setFormatter(simple_formatter)
    root_logger.addHandler(console_handler)
    
    # File handler for all logs
    file_handler = logging.handlers.RotatingFileHandler(
        os.path.join(logs_dir, 'application.log'),
        maxBytes=50*1024*1024,  # 50MB
        backupCount=5
    )
    file_handler.setLevel(logging.INFO)
    file_handler.setFormatter(detailed_formatter)
    root_logger.addHandler(file_handler)
    
    # Error file handler
    error_handler = logging.handlers.RotatingFileHandler(
        os.path.join(logs_dir, 'errors.log'),
        maxBytes=10*1024*1024,  # 10MB
        backupCount=3
    )
    error_handler.setLevel(logging.ERROR)
    error_handler.setFormatter(detailed_formatter)
    root_logger.addHandler(error_handler)
    
    # Analysis specific logger
    analysis_logger = logging.getLogger('analysis')
    analysis_handler = logging.handlers.RotatingFileHandler(
        os.path.join(logs_dir, 'analysis.log'),
        maxBytes=20*1024*1024,  # 20MB
        backupCount=3
    )
    analysis_handler.setFormatter(detailed_formatter)
    analysis_logger.addHandler(analysis_handler)
    
    # Data collection logger
    datacollection_logger = logging.getLogger('datacollection')
    datacollection_handler = logging.handlers.RotatingFileHandler(
        os.path.join(logs_dir, 'datacollection.log'),
        maxBytes=20*1024*1024,  # 20MB
        backupCount=3
    )
    datacollection_handler.setFormatter(detailed_formatter)
    datacollection_logger.addHandler(datacollection_handler)

# Performance monitoring decorator
def monitor_performance(func):
    def wrapper(*args, **kwargs):
        start_time = datetime.now()
        logger = logging.getLogger(f"performance.{func.__module__}")
        
        try:
            result = func(*args, **kwargs)
            execution_time = (datetime.now() - start_time).total_seconds()
            
            logger.info(f"{func.__name__} completed in {execution_time:.2f}s")
            return result
            
        except Exception as e:
            execution_time = (datetime.now() - start_time).total_seconds()
            logger.error(f"{func.__name__} failed after {execution_time:.2f}s: {str(e)}")
            raise
    
    return wrapper
\end{lstlisting}

%=====================================
% API DOCUMENTATION
%=====================================
\chapter{API Documentation}

\section{Authentication Endpoints}

\subsection{User Registration}

\textbf{POST} \texttt{/api/register}

Registra un nuovo utente nel sistema.

\begin{lstlisting}[language=json, caption=Request Body]
{
  "username": "researcher1",
  "email": "researcher@university.edu",
  "password": "secure_password123"
}
\end{lstlisting}

\begin{lstlisting}[language=json, caption=Response (201 Created)]
{
  "message": "Utente registrato con successo",
  "user_id": "64f7a8b2c1234567890abcdef"
}
\end{lstlisting}

\subsection{User Login}

\textbf{POST} \texttt{/api/login}

Autentica un utente e restituisce un token JWT.

\begin{lstlisting}[language=json, caption=Request Body]
{
  "username": "researcher1",
  "password": "secure_password123"
}
\end{lstlisting}

\begin{lstlisting}[language=json, caption=Response (200 OK)]
{
  "token": "eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9...",
  "user": {
    "id": "64f7a8b2c1234567890abcdef",
    "username": "researcher1",
    "email": "researcher@university.edu",
    "role": "researcher"
  }
}
\end{lstlisting}

\section{Campaign Management Endpoints}

\subsection{Create Campaign}

\textbf{POST} \texttt{/api/campaigns}

Crea una nuova campagna di monitoraggio.

\textbf{Headers:} \texttt{Authorization: Bearer <token>}

\begin{lstlisting}[language=json, caption=Request Body]
{
  "name": "Monitoraggio Vaccini COVID-19",
  "keywords": ["vaccino", "covid", "pfizer", "moderna"],
  "social_platforms": ["twitter", "reddit", "youtube"]
}
\end{lstlisting}

\begin{lstlisting}[language=json, caption=Response (201 Created)]
{
  "message": "Campagna creata con successo!",
  "campaign_id": "64f7a8b2c1234567890abcdef",
  "campaign": {
    "name": "Monitoraggio Vaccini COVID-19",
    "start_date": "2025-01-15T10:30:00Z",
    "social_platforms": ["twitter", "reddit", "youtube"],
    "keywords": ["vaccino", "covid", "pfizer", "moderna"]
  }
}
\end{lstlisting}

\subsection{Get User Campaigns}

\textbf{GET} \texttt{/api/campaigns}

Recupera tutte le campagne dell'utente autenticato.

\textbf{Headers:} \texttt{Authorization: Bearer <token>}

\begin{lstlisting}[language=json, caption=Response (200 OK)]
{
  "campaigns": [
    {
      "id": "64f7a8b2c1234567890abcdef",
      "name": "Monitoraggio Vaccini COVID-19",
      "start_date": "2025-01-15T10:30:00Z",
      "social_platforms": ["twitter", "reddit"],
      "keywords": ["vaccino", "covid"],
      "status": "active",
      "created_at": "2025-01-15T10:30:00Z",
      "last_updated": "2025-01-15T12:00:00Z"
    }
  ]
}
\end{lstlisting}

\subsection{Get Campaign Details}

\textbf{GET} \texttt{/api/campaigns/\{campaign\_id\}}

Recupera i dettagli e i post di una campagna specifica.

\textbf{Headers:} \texttt{Authorization: Bearer <token>}

\begin{lstlisting}[language=json, caption=Response (200 OK)]
{
  "campaign": {
    "id": "64f7a8b2c1234567890abcdef",
    "name": "Monitoraggio Vaccini COVID-19",
    "keywords": ["vaccino", "covid"],
    "social_platforms": ["twitter", "reddit"],
    "status": "active",
    "start_date": "2025-01-15T10:30:00Z"
  },
  "posts": [
    {
      "id": "64f7a8b2c1234567890abcde1",
      "platform": "twitter",
      "author": {
        "name": "user123",
        "verified": false
      },
      "content": "Il vaccino COVID-19 contiene microchip...",
      "url": "https://twitter.com/user123/status/123456789",
      "score": 45,
      "num_comments": 12,
      "created_at": "2025-01-15T11:00:00Z",
      "keyword": "vaccino",
      "analysis_results": {
        "grado_disinformazione": 4,
        "sentiment": "negative",
        "confidence": 0.85
      },
      "is_fake": "fake"
    }
  ]
}
\end{lstlisting}

\section{Post Analysis Endpoints}

\subsection{Get Post Details}

\textbf{GET} \texttt{/api/posts/\{post\_id\}}

Recupera i dettagli completi di un post specifico.

\textbf{Headers:} \texttt{Authorization: Bearer <token>}

\begin{lstlisting}[language=json, caption=Response (200 OK)]
{
  "post": {
    "id": "64f7a8b2c1234567890abcde1",
    "campaign_id": "64f7a8b2c1234567890abcdef",
    "platform": "twitter",
    "content": "Il vaccino COVID-19 contiene microchip per il controllo mentale...",
    "author_name": "user123",
    "url": "https://twitter.com/user123/status/123456789",
    "created_at": "2025-01-15T11:00:00Z",
    "analysis_results": {
      "grado_disinformazione": 4,
      "sentiment": "negative",
      "confidence": 0.85,
      "factcheck": {
        "general_claim": {
          "verdict": "FAKE",
          "reasoning": "Non esistono evidenze scientifiche di microchip nei vaccini COVID-19",
          "cited_evidence": [
            {
              "idx": 1,
              "title": "COVID-19 vaccine composition analysis",
              "url": "https://pubmed.ncbi.nlm.nih.gov/12345678"
            }
          ]
        },
        "overall_verdict": "FAKE"
      }
    },
    "medical_concepts": [
      {
        "text": "vaccino",
        "type": "drug",
        "confidence": 0.9
      },
      {
        "text": "COVID-19",
        "type": "medical_condition",
        "confidence": 0.95
      }
    ],
    "related_posts": [
      {
        "id": "64f7a8b2c1234567890abcde2",
        "content": "Anche io ho sentito della storia dei microchip...",
        "platform": "reddit",
        "fake_score": 3
      }
    ],
    "fact_check_sources": [
      {
        "url": "https://pubmed.ncbi.nlm.nih.gov/12345678",
        "title": "COVID-19 vaccine composition analysis",
        "type": "pubmed"
      }
    ]
  }
}
\end{lstlisting}

\section{Analysis Endpoints}

\subsection{Trigger Manual Analysis}

\textbf{POST} \texttt{/api/analysis/analyze}

Avvia l'analisi manuale di un testo specifico.

\textbf{Headers:} \texttt{Authorization: Bearer <token>}

\begin{lstlisting}[language=json, caption=Request Body]
{
  "text": "Il nuovo farmaco XYZ cura il cancro al 100%",
  "topic": "oncologia"
}
\end{lstlisting}

\begin{lstlisting}[language=json, caption=Response (200 OK)]
{
  "status": "ok",
  "analysis_id": "64f7a8b2c1234567890abcde3",
  "factcheck": {
    "general_claim": {
      "verdict": "FAKE",
      "reasoning": "Nessun farmaco attualmente disponibile garantisce una cura al 100% per il cancro",
      "cited_evidence": [
        {
          "idx": 1,
          "title": "Current cancer treatment efficacy rates",
          "url": "https://pubmed.ncbi.nlm.nih.gov/87654321"
        }
      ]
    },
    "overall_verdict": "FAKE",
    "confidence": 0.92
  },
  "grado_disinformazione": 4,
  "sentiment": "positive",
  "medical_concepts": [
    {
      "text": "farmaco",
      "type": "drug",
      "confidence": 0.88
    },
    {
      "text": "cancro",
      "type": "medical_condition",
      "confidence": 0.95
    }
  ]
}
\end{lstlisting}

\section{Report Generation Endpoints}

\subsection{Generate Campaign Report}

\textbf{POST} \texttt{/api/reports/campaign/\{campaign\_id\}}

Genera un report completo per una campagna specifica.

\textbf{Headers:} \texttt{Authorization: Bearer <token>}

\begin{lstlisting}[language=json, caption=Request Body]
{
  "format": "pdf",
  "include_charts": true,
  "date_range": {
    "start": "2025-01-01",
    "end": "2025-01-31"
  }
}
\end{lstlisting}

\begin{lstlisting}[language=json, caption=Response (200 OK)]
{
  "report_id": "64f7a8b2c1234567890abcde4",
  "status": "completed",
  "download_url": "/api/reports/download/64f7a8b2c1234567890abcde4",
  "summary": {
    "total_posts": 1250,
    "fake_posts": 156,
    "fake_percentage": 12.48,
    "platforms_breakdown": {
      "twitter": 680,
      "reddit": 370,
      "youtube": 200
    },
    "top_authors": [
      {
        "name": "suspicious_user1",
        "post_count": 45,
        "avg_fake_score": 3.2
      }
    ]
  }
}
\end{lstlisting}

\section{Error Handling}

Il sistema utilizza codici di stato HTTP standard e fornisce messaggi di errore strutturati:

\begin{lstlisting}[language=json, caption=Error Response Format]
{
  "error": {
    "code": "INVALID_REQUEST",
    "message": "Descrizione dell'errore in italiano",
    "details": "Informazioni aggiuntive opzionali",
    "timestamp": "2025-01-15T12:00:00Z"
  }
}
\end{lstlisting}

\textbf{Codici di Errore Comuni:}
\begin{table}[H]
\centering
\begin{tabular}{@{}lll@{}}
\toprule
\textbf{Codice HTTP} & \textbf{Errore} & \textbf{Descrizione} \\
\midrule
400 & BAD\_REQUEST & Richiesta malformata o parametri mancanti \\
401 & UNAUTHORIZED & Token JWT mancante o non valido \\
403 & FORBIDDEN & Accesso negato per il ruolo utente \\
404 & NOT\_FOUND & Risorsa non trovata \\
429 & RATE\_LIMITED & Troppi tentativi, rate limit superato \\
500 & INTERNAL\_ERROR & Errore interno del server \\
503 & SERVICE\_UNAVAILABLE & Servizio temporaneamente non disponibile \\
\bottomrule
\end{tabular}
\caption{Codici di Errore API}
\label{tab:api-errors}
\end{table}

%=====================================
% SICUREZZA E PRIVACY
%=====================================
\chapter{Sicurezza e Privacy}

\section{Misure di Sicurezza Implementate}

\subsection{Autenticazione e Autorizzazione}

Il sistema implementa un robusto sistema di sicurezza multi-livello:

\textbf{JWT Token Management:}
\begin{itemize}
    \item Token JWT con scadenza configurabile (default: 24 ore)
    \item Algoritmo di firma HMAC SHA-256
    \item Refresh token per sessioni prolungate
    \item Invalidazione automatica dei token compromessi
\end{itemize}

\textbf{Password Security:}
\begin{itemize}
    \item Hashing con bcrypt e salt casuali
    \item Politiche di password complesse (minimo 8 caratteri, maiuscole, numeri)
    \item Rate limiting sui tentativi di login
    \item Account lockout dopo tentativi falliti
\end{itemize}

\textbf{Role-Based Access Control (RBAC):}
\begin{itemize}
    \item Ruoli: \texttt{admin}, \texttt{researcher}, \texttt{viewer}
    \item Permessi granulari per ogni endpoint
    \item Isolamento dei dati tra utenti
\end{itemize}

\subsection{Sicurezza delle Comunicazioni}

\textbf{Transport Layer Security:}
\begin{itemize}
    \item TLS 1.3 obbligatorio per tutte le comunicazioni
    \item Certificate pinning per API esterne
    \item HSTS (HTTP Strict Transport Security) headers
    \item Perfect Forward Secrecy
\end{itemize}

\textbf{API Security:}
\begin{itemize}
    \item Rate limiting per endpoint (configurabile)
    \item Request validation e sanitizzazione input
    \item CORS policy restrittiva
    \item Content Security Policy (CSP) headers
\end{itemize}

\subsection{Sicurezza dei Dati}

\textbf{Database Security:}
\begin{itemize}
    \item Connessioni MongoDB cifrate (TLS/SSL)
    \item Segregazione database per ambiente (dev/staging/prod)
    \item Backup cifrati con rotazione automatica
    \item Audit logging di tutte le operazioni sensibili
\end{itemize}

\textbf{API Keys e Secrets:}
\begin{itemize}
    \item Gestione sicura delle chiavi API esterne
    \item Rotazione periodica delle credenziali
    \item Ambiente variabili mai committate nel codice
    \item Encryption at rest per secrets sensibili
\end{itemize}

\section{Gestione della Privacy}

\subsection{Conformità GDPR}

Il sistema è progettato per essere conforme al Regolamento Generale sulla Protezione dei Dati:

\textbf{Principi di Privacy by Design:}
\begin{itemize}
    \item Minimizzazione dei dati raccolti
    \item Pseudonimizzazione automatica dei dati personali
    \item Cancellazione automatica dopo periodo di retention
    \item Consent management per utenti EU
\end{itemize}

\textbf{Diritti degli Interessati:}
\begin{itemize}
    \item Right to access: API per esportazione dati utente
    \item Right to rectification: Modifica dati personali
    \item Right to erasure: Cancellazione account e dati correlati
    \item Right to portability: Export in formato standard
\end{itemize}

\subsection{Anonimizzazione dei Dati Social}

\begin{lstlisting}[language=python, caption=Data Anonymization Service]
import hashlib
import re
from typing import Dict, Any

class DataAnonymizer:
    def __init__(self):
        self.salt = os.environ.get('ANONYMIZATION_SALT', 'default-salt')
    
    def anonymize_social_post(self, post_data: Dict[str, Any]) -> Dict[str, Any]:
        """Anonimizza i dati sensibili di un post social"""
        anonymized_post = post_data.copy()
        
        # Anonimizza username
        if 'author_name' in anonymized_post:
            anonymized_post['author_name'] = self._hash_username(
                anonymized_post['author_name']
            )
        
        # Rimuovi metadati sensibili
        sensitive_fields = ['author_id', 'user_email', 'ip_address', 'geolocation']
        for field in sensitive_fields:
            anonymized_post.pop(field, None)
        
        # Anonimizza contenuto del testo mantenendo utilità per analisi
        if 'text' in anonymized_post:
            anonymized_post['text'] = self._anonymize_text(anonymized_post['text'])
        
        # Anonimizza URL mantenendo dominio per analisi
        if 'url' in anonymized_post:
            anonymized_post['url'] = self._anonymize_url(anonymized_post['url'])
        
        return anonymized_post
    
    def _hash_username(self, username: str) -> str:
        """Genera hash consistente per username"""
        return hashlib.sha256(f"{username}{self.salt}".encode()).hexdigest()[:16]
    
    def _anonymize_text(self, text: str) -> str:
        """Anonimizza PII nel testo mantenendo contenuto medico"""
        # Rimuovi email
        text = re.sub(r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b', '[EMAIL]', text)
        
        # Rimuovi numeri di telefono
        text = re.sub(r'\b\d{3}[-.]?\d{3}[-.]?\d{4}\b', '[PHONE]', text)
        
        # Rimuovi nomi propri (pattern semplificato)
        text = re.sub(r'\b[A-Z][a-z]+ [A-Z][a-z]+\b', '[NAME]', text)
        
        # Mantieni terminologia medica
        return text
    
    def _anonymize_url(self, url: str) -> str:
        """Mantiene dominio ma rimuove path specifici"""
        parsed = urlparse(url)
        return f"{parsed.scheme}://{parsed.netloc}/[PATH_REMOVED]"

# Data Retention Policy
class DataRetentionManager:
    def __init__(self, mongo_manager):
        self.db = mongo_manager.db
        self.retention_periods = {
            'social_posts': 365,  # 1 anno
            'user_sessions': 30,   # 30 giorni
            'analysis_logs': 90,   # 3 mesi
            'audit_logs': 2555     # 7 anni (compliance)
        }
    
    def cleanup_expired_data(self):
        """Rimuove dati scaduti secondo policy retention"""
        current_date = datetime.now()
        
        for collection_name, retention_days in self.retention_periods.items():
            cutoff_date = current_date - timedelta(days=retention_days)
            
            try:
                result = self.db[collection_name].delete_many({
                    'created_at': {'$lt': cutoff_date}
                })
                
                logging.info(f"Removed {result.deleted_count} expired records from {collection_name}")
                
            except Exception as e:
                logging.error(f"Error cleaning up {collection_name}: {e}")
\end{lstlisting}

\section{Audit e Compliance}

\subsection{Audit Logging}

Tutte le operazioni sensibili sono registrate per compliance:

\begin{lstlisting}[language=python, caption=Audit Logging System]
import json
from datetime import datetime
from enum import Enum

class AuditEventType(Enum):
    USER_LOGIN = "user_login"
    USER_LOGOUT = "user_logout"
    CAMPAIGN_CREATED = "campaign_created"
    CAMPAIGN_DELETED = "campaign_deleted"
    DATA_EXPORT = "data_export"
    ANALYSIS_TRIGGERED = "analysis_triggered"
    ADMIN_ACTION = "admin_action"

class AuditLogger:
    def __init__(self, mongo_manager):
        self.audit_collection = mongo_manager.db.audit_logs
    
    def log_event(self, event_type: AuditEventType, user_id: str, 
                  details: dict = None, ip_address: str = None):
        """Registra un evento di audit"""
        
        audit_entry = {
            'timestamp': datetime.now(),
            'event_type': event_type.value,
            'user_id': user_id,
            'ip_address': ip_address,
            'details': details or {},
            'session_id': self._get_current_session_id(),
            'user_agent': request.headers.get('User-Agent') if request else None
        }
        
        try:
            self.audit_collection.insert_one(audit_entry)
        except Exception as e:
            # Audit logging failures sono critici ma non devono bloccare l'app
            logging.critical(f"AUDIT LOGGING FAILED: {e}")
    
    def get_user_audit_trail(self, user_id: str, start_date: datetime = None, 
                           end_date: datetime = None) -> list:
        """Recupera audit trail per un utente specifico"""
        
        query = {'user_id': user_id}
        
        if start_date or end_date:
            date_filter = {}
            if start_date:
                date_filter['$gte'] = start_date
            if end_date:
                date_filter['$lte'] = end_date
            query['timestamp'] = date_filter
        
        return list(self.audit_collection.find(query).sort('timestamp', -1))

# Usage in routes
@auth_bp.route('/login', methods=['POST'])
def login():
    # ... authentication logic ...
    
    if authenticated:
        audit_logger.log_event(
            AuditEventType.USER_LOGIN,
            user_id=user['id'],
            details={'username': username, 'success': True},
            ip_address=request.remote_addr
        )
    else:
        audit_logger.log_event(
            AuditEventType.USER_LOGIN,
            user_id=None,
            details={'username': username, 'success': False, 'reason': 'invalid_credentials'},
            ip_address=request.remote_addr
        )
\end{lstlisting}

%=====================================
% TESTING E QUALITÀ
%=====================================
\chapter{Testing e Qualità del Software}

\section{Strategia di Testing}

\subsection{Pyramid Testing Strategy}

Il sistema adotta una strategia di testing piramidale con focus su:

\begin{enumerate}
    \item \textbf{Unit Tests (70\%)}: Test di singole funzioni e metodi
    \item \textbf{Integration Tests (20\%)}: Test di interazione tra componenti
    \item \textbf{E2E Tests (10\%)}: Test dell'intero flusso utente
\end{enumerate}

\subsection{Framework di Testing}

\textbf{Backend Testing (Python):}
\begin{itemize}
    \item \textbf{pytest}: Framework principale per unit e integration tests
    \item \textbf{pytest-mock}: Mocking per dipendenze esterne
    \item \textbf{pytest-cov}: Coverage reporting
    \item \textbf{factory\_boy}: Generazione dati di test
\end{itemize}

\textbf{Frontend Testing (React):}
\begin{itemize}
    \item \textbf{Jest}: Unit testing framework
    \item \textbf{React Testing Library}: Component testing
    \item \textbf{Cypress}: End-to-end testing
    \item \textbf{MSW}: API mocking per testing
\end{itemize}

\section{Test Implementation Examples}

\subsection{Backend Unit Tests}

\begin{lstlisting}[language=python, caption=Example Unit Tests - FactCheck Service]
import pytest
from unittest.mock import Mock, patch
from app.services.analysis.factcheck_service import FactCheckService

class TestFactCheckService:
    
    @pytest.fixture
    def factcheck_service(self):
        """Setup FactCheckService with mocked dependencies"""
        with patch('app.services.llm.llm_manager.LLMManager') as mock_llm:
            service = FactCheckService()
            service.llm_manager = mock_llm
            return service
    
    @pytest.fixture
    def sample_post_text(self):
        return "Il vaccino COVID-19 contiene microchip per il controllo mentale"
    
    @pytest.fixture
    def sample_evidence_chunks(self):
        return [
            {
                'content': 'COVID-19 vaccines contain mRNA or viral proteins, not microchips',
                'meta': {
                    'url': 'https://pubmed.ncbi.nlm.nih.gov/12345',
                    'source': 'pubmed'
                }
            }
        ]
    
    def test_run_factcheck_success(self, factcheck_service, sample_post_text, sample_evidence_chunks):
        """Test successful fact-checking with valid inputs"""
        # Arrange
        expected_result = {
            'general_claim': {
                'verdict': 'FAKE',
                'reasoning': 'No evidence of microchips in vaccines',
                'cited_evidence': [{'idx': 1, 'url': 'https://pubmed.ncbi.nlm.nih.gov/12345'}]
            },
            'overall_verdict': 'FAKE',
            'confidence': 0.9
        }
        
        factcheck_service.llm_manager.factcheck_with_retry.return_value = expected_result
        
        # Act
        result = factcheck_service.run_factcheck(
            topic="vaccini",
            post_text=sample_post_text
        )
        
        # Assert
        assert result['status'] == 'ok'
        assert result['factcheck']['overall_verdict'] == 'FAKE'
        assert result['grado_disinformazione'] >= 3
        assert 'fonti_utilizzate' in result
    
    def test_run_factcheck_empty_input(self, factcheck_service):
        """Test fact-checking with empty inputs"""
        # Act
        result = factcheck_service.run_factcheck(topic="", post_text="")
        
        # Assert
        assert result['status'] == 'error'
        assert 'missing_input' in result['message']
    
    def test_normalize_topic_string(self, factcheck_service):
        """Test topic normalization with string input"""
        # Act
        normalized = factcheck_service._normalize_topic("vaccini")
        
        # Assert
        assert normalized == "vaccini"
        assert isinstance(normalized, str)
    
    def test_normalize_topic_list(self, factcheck_service):
        """Test topic normalization with list input"""
        # Act
        normalized = factcheck_service._normalize_topic(["vaccini", "covid"])
        
        # Assert
        assert normalized == "vaccini covid"
        assert isinstance(normalized, str)
    
    @patch('app.services.data_collection.evergreen.evergreen_service.EverGreenService')
    def test_collect_evidence_success(self, mock_evergreen, factcheck_service):
        """Test evidence collection with mocked services"""
        # Arrange
        mock_evergreen.return_value.get_evergreen_for_topic.return_value = [
            {
                'text': 'Scientific evidence about vaccines',
                'url': 'https://example.com/article1',
                'title': 'Vaccine Safety Study'
            }
        ]
        
        # Act
        evidence = factcheck_service._collect_evidence("vaccini", "test post")
        
        # Assert
        assert isinstance(evidence, list)
        mock_evergreen.assert_called_once()

# Integration Tests
class TestFactCheckIntegration:
    
    @pytest.fixture
    def app_context(self):
        """Setup Flask app context for integration tests"""
        from app import create_app
        app = create_app('testing')
        with app.app_context():
            yield app
    
    def test_full_factcheck_pipeline(self, app_context):
        """Test complete fact-checking pipeline integration"""
        from app.services.analysis.factcheck_service import FactCheckService
        
        # Arrange
        service = FactCheckService()
        test_post = "L'omeopatia cura il cancro meglio della chemioterapia"
        
        # Act (this would use real APIs in integration environment)
        with patch.multiple(
            'app.services.llm.providers.openai_service.OpenAIService',
            factcheck_with_evidence=Mock(return_value={
                'general_claim': {'verdict': 'FAKE'},
                'overall_verdict': 'FAKE',
                'confidence': 0.85
            })
        ):
            result = service.run_factcheck("medicina alternativa", test_post)
        
        # Assert
        assert result['status'] == 'ok'
        assert 'factcheck' in result
        assert 'grado_disinformazione' in result

# Performance Tests
class TestFactCheckPerformance:
    
    def test_factcheck_response_time(self, factcheck_service):
        """Test that fact-checking completes within acceptable time"""
        import time
        
        start_time = time.time()
        
        # Mock fast response
        factcheck_service.llm_manager.factcheck_with_retry.return_value = {
            'overall_verdict': 'UNCERTAIN',
            'confidence': 0.5
        }
        
        result = factcheck_service.run_factcheck("test", "test post")
        
        end_time = time.time()
        execution_time = end_time - start_time
        
        # Should complete within 5 seconds (including mocking overhead)
        assert execution_time < 5.0
        assert result['status'] == 'ok'
\end{lstlisting}

\subsection{Frontend Component Tests}

\begin{lstlisting}[language=javascript, caption=Example React Component Tests]
import React from 'react';
import { render, screen, fireEvent, waitFor } from '@testing-library/react';
import { BrowserRouter } from 'react-router-dom';
import { AuthProvider } from '../context/AuthContext';
import CreateCampaignModal from '../components/CreateCampaignModal';

// Mock fetch globally
global.fetch = jest.fn();

const renderWithProviders = (component) => {
  return render(
    <BrowserRouter>
      <AuthProvider>
        {component}
      </AuthProvider>
    </BrowserRouter>
  );
};

describe('CreateCampaignModal', () => {
  const mockOnCreate = jest.fn();
  const mockOnClose = jest.fn();

  beforeEach(() => {
    jest.clearAllMocks();
  });

  test('renders modal when open', () => {
    renderWithProviders(
      <CreateCampaignModal 
        isOpen={true} 
        onClose={mockOnClose} 
        onCreate={mockOnCreate} 
      />
    );

    expect(screen.getByText('Crea Nuova Campagna')).toBeInTheDocument();
    expect(screen.getByLabelText(/nome campagna/i)).toBeInTheDocument();
    expect(screen.getByLabelText(/parole chiave/i)).toBeInTheDocument();
  });

  test('does not render when closed', () => {
    renderWithProviders(
      <CreateCampaignModal 
        isOpen={false} 
        onClose={mockOnClose} 
        onCreate={mockOnCreate} 
      />
    );

    expect(screen.queryByText('Crea Nuova Campagna')).not.toBeInTheDocument();
  });

  test('validates required fields', async () => {
    renderWithProviders(
      <CreateCampaignModal 
        isOpen={true} 
        onClose={mockOnClose} 
        onCreate={mockOnCreate} 
      />
    );

    // Try to submit without filling required fields
    const submitButton = screen.getByText('Crea Campagna');
    fireEvent.click(submitButton);

    await waitFor(() => {
      expect(screen.getByText('Nome campagna richiesto')).toBeInTheDocument();
      expect(screen.getByText('Almeno una parola chiave richiesta')).toBeInTheDocument();
      expect(screen.getByText('Seleziona almeno una piattaforma')).toBeInTheDocument();
    });

    expect(mockOnCreate).not.toHaveBeenCalled();
  });

  test('submits valid form data', async () => {
    renderWithProviders(
      <CreateCampaignModal 
        isOpen={true} 
        onClose={mockOnClose} 
        onCreate={mockOnCreate} 
      />
    );

    // Fill form fields
    fireEvent.change(screen.getByLabelText(/nome campagna/i), {
      target: { value: 'Test Campaign' }
    });

    fireEvent.change(screen.getByLabelText(/parole chiave/i), {
      target: { value: 'vaccino, covid, salute' }
    });

    // Select platforms
    fireEvent.click(screen.getByLabelText(/twitter/i));
    fireEvent.click(screen.getByLabelText(/reddit/i));

    // Submit form
    const submitButton = screen.getByText('Crea Campagna');
    fireEvent.click(submitButton);

    await waitFor(() => {
      expect(mockOnCreate).toHaveBeenCalledWith({
        name: 'Test Campaign',
        keywords: ['vaccino', 'covid', 'salute'],
        social_platforms: ['twitter', 'reddit']
      });
    });
  });

  test('handles platform selection correctly', () => {
    renderWithProviders(
      <CreateCampaignModal 
        isOpen={true} 
        onClose={mockOnClose} 
        onCreate={mockOnCreate} 
      />
    );

    const twitterCheckbox = screen.getByLabelText(/twitter/i);
    const redditCheckbox = screen.getByLabelText(/reddit/i);

    // Select Twitter
    fireEvent.click(twitterCheckbox);
    expect(twitterCheckbox).toBeChecked();

    // Select Reddit
    fireEvent.click(redditCheckbox);
    expect(redditCheckbox).toBeChecked();

    // Deselect Twitter
    fireEvent.click(twitterCheckbox);
    expect(twitterCheckbox).not.toBeChecked();
    expect(redditCheckbox).toBeChecked();
  });

  test('shows loading state during submission', async () => {
    // Mock slow response
    mockOnCreate.mockImplementation(() => new Promise(resolve => setTimeout(resolve, 1000)));

    renderWithProviders(
      <CreateCampaignModal 
        isOpen={true} 
        onClose={mockOnClose} 
        onCreate={mockOnCreate} 
      />
    );

    // Fill required fields
    fireEvent.change(screen.getByLabelText(/nome campagna/i), {
      target: { value: 'Test Campaign' }
    });
    fireEvent.change(screen.getByLabelText(/parole chiave/i), {
      target: { value: 'test' }
    });
    fireEvent.click(screen.getByLabelText(/twitter/i));

    // Submit form
    const submitButton = screen.getByText('Crea Campagna');
    fireEvent.click(submitButton);

    // Check loading state
    expect(screen.getByText('Creazione...')).toBeInTheDocument();
    expect(submitButton).toBeDisabled();
  });
});

// E2E Test Example (Cypress)
describe('Campaign Management E2E', () => {
  beforeEach(() => {
    // Login user
    cy.login('testuser', 'password123');
    cy.visit('/');
  });

  it('creates a new campaign successfully', () => {
    // Click create campaign button
    cy.get('[data-testid="create-campaign-btn"]').click();

    // Fill campaign form
    cy.get('input[name="name"]').type('E2E Test Campaign');
    cy.get('textarea[name="keywords"]').type('test, e2e, automation');
    cy.get('input[value="twitter"]').check();
    cy.get('input[value="reddit"]').check();

    // Submit form
    cy.get('button[type="submit"]').click();

    // Verify campaign was created
    cy.get('.success-message').should('contain', 'Campagna creata con successo');
    cy.get('.campaign-card').should('contain', 'E2E Test Campaign');
  });

  it('displays campaign analysis results', () => {
    // Navigate to campaign with existing data
    cy.get('.campaign-card').first().click();

    // Check analysis results are displayed
    cy.get('.post-item').should('have.length.greaterThan', 0);
    cy.get('.fake-score').should('be.visible');
    cy.get('.sentiment-indicator').should('be.visible');

    // Check detailed view
    cy.get('.post-item').first().click();
    cy.get('.analysis-details').should('be.visible');
    cy.get('.evidence-sources').should('be.visible');
  });
});
\end{lstlisting}

\section{Metriche di Qualità}

\subsection{Code Coverage}

Target di copertura del codice:
\begin{itemize}
    \item \textbf{Unit Tests}: > 85\% line coverage
    \item \textbf{Integration Tests}: > 70\% path coverage
    \item \textbf{Critical Paths}: 100\% coverage (autenticazione, fact-checking)
\end{itemize}

\subsection{Performance Metrics}

Benchmark di performance per componenti critici:

\begin{table}[H]
\centering
\begin{tabular}{@{}lcc@{}}
\toprule
\textbf{Operazione} & \textbf{Target} & \textbf{Misurato} \\
\midrule
Fact-check singolo post & < 3s & 2.1s \\
Raccolta 100 post Twitter & < 30s & 25s \\
Analisi campagna (1000 post) & < 5min & 4.2min \\
Caricamento dashboard & < 2s & 1.8s \\
Export report PDF & < 10s & 8.5s \\
\bottomrule
\end{tabular}
\caption{Performance Benchmarks}
\label{tab:performance-metrics}
\end{table}

\subsection{Quality Gates}

Criteri di qualità che devono essere soddisfatti prima del deploy:

\begin{enumerate}
    \item Tutti i test devono passare (0 failures)
    \item Code coverage > 85\%
    \item Nessuna vulnerabilità di sicurezza critica (OWASP scan)
    \item Performance tests entro i limiti definiti
    \item Code review approvato da almeno 2 reviewer
    \item Documentazione aggiornata
\end{enumerate}

\section{Continuous Integration Pipeline}

\begin{lstlisting}[language=yaml, caption=GitHub Actions CI/CD Pipeline]
name: CI/CD Pipeline

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main ]

jobs:
  test-backend:
    runs-on: ubuntu-latest
    
    services:
      mongodb:
        image: mongo:5.0
        ports:
          - 27017:27017
      redis:
        image: redis:6.2
        ports:
          - 6379:6379
      elasticsearch:
        image: docker.elastic.co/elasticsearch/elasticsearch:8.5.0
        env:
          discovery.type: single-node
          xpack.security.enabled: false
        ports:
          - 9200:9200

    steps:
    - uses: actions/checkout@v3
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.9'
    
    - name: Install dependencies
      run: |
        cd backend
        pip install -r requirements.txt
        python -m spacy download it_core_news_lg
    
    - name: Run linting
      run: |
        cd backend
        flake8 app/ --count --select=E9,F63,F7,F82 --show-source --statistics
        black --check app/
    
    - name: Run unit tests
      run: |
        cd backend
        pytest tests/unit/ --cov=app --cov-report=xml --cov-report=html
    
    - name: Run integration tests
      run: |
        cd backend
        pytest tests/integration/ --cov=app --cov-append --cov-report=xml
    
    - name: Upload coverage to Codecov
      uses: codecov/codecov-action@v3
      with:
        file: backend/coverage.xml

  test-frontend:
    runs-on: ubuntu-latest
    
    steps:
    - uses: actions/checkout@v3
    
    - name: Set up Node.js
      uses: actions/setup-node@v3
      with:
        node-version: '18'
        cache: 'npm'
        cache-dependency-path: frontend/package-lock.json
    
    - name: Install dependencies
      run: |
        cd frontend
        npm ci
    
    - name: Run linting
      run: |
        cd frontend
        npm run lint
    
    - name: Run unit tests
      run: |
        cd frontend
        npm run test:ci
    
    - name: Build production
      run: |
        cd frontend
        npm run build

  security-scan:
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v3
    
    - name: Run security scan
      uses: securecodewarrior/github-action-add-sarif@v1
      with:
        sarif-file: 'security-scan-results.sarif'
    
    - name: OWASP ZAP Scan
      uses: zaproxy/action-full-scan@v0.4.0
      with:
        target: 'http://localhost:5000'

  deploy:
    needs: [test-backend, test-frontend, security-scan]
    runs-on: ubuntu-latest
    if: github.ref == 'refs/heads/main'
    
    steps:
    - uses: actions/checkout@v3
    
    - name: Deploy to production
      run: |
        echo "Deploying to production environment..."
        # Production deployment steps here
\end{lstlisting}

\end{document}